{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def register_datapipeline(name):\n    \"\"\"Decorator used register a CARP architecture\n    Args:\n        name: Name of the architecture\n    \"\"\"\n    def register_class(cls, name):\n        _DATAPIPELINE[name] = cls\n        setattr(sys.modules[__name__], name, cls)\n        return cls\n\n    if isinstance(name, str):\n        name = name.lower()\n        return lambda c: register_class(c, name)\n\n    cls = name\n    name = cls.__name__\n    register_class(cls, name.lower())\n\n    return cls\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "pipeline", "__init__.py"], "context_start_lineno": 0, "lineno": 19, "function_name": "register_datapipeline"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def register_class(cls, name):\n        _DATAPIPELINE[name] = cls\n        setattr(sys.modules[__name__], name, cls)\n        return cls\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "pipeline", "__init__.py"], "context_start_lineno": 0, "lineno": 20, "function_name": "register_class"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def load_yaml(cls, yml_fp: str):\n        \"\"\"\n        Load yaml file as TRLConfig.\n\n        :param yml_fp: Path to yaml file\n        :type yml_fp: str\n        \"\"\"\n        with open(yml_fp, mode=\"r\") as file:\n            config = yaml.safe_load(file)\n        return cls.from_dict(config)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "data", "configs.py"], "context_start_lineno": 0, "lineno": 234, "function_name": "load_yaml"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def from_dict(cls, config: Dict):\n        \"\"\"\n        Convert dictionary to TRLConfig.\n        \"\"\"\n        return cls(\n            method=get_method(config[\"method\"][\"name\"]).from_dict(config[\"method\"]),\n            model=ModelConfig.from_dict(config[\"model\"]),\n            tokenizer=TokenizerConfig.from_dict(config[\"tokenizer\"]),\n            optimizer=OptimizerConfig.from_dict(config[\"optimizer\"]),\n            scheduler=SchedulerConfig.from_dict(config[\"scheduler\"]),\n            train=TrainConfig.from_dict(config[\"train\"]),\n        )\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "data", "configs.py"], "context_start_lineno": 26, "lineno": 258, "function_name": "from_dict"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def register_method(name):\n    \"\"\"Decorator used register a method config\n    Args:\n        name: Name of the method\n    \"\"\"\n    def register_class(cls, name):\n        _METHODS[name] = cls\n        setattr(sys.modules[__name__], name, cls)\n        return cls\n\n    if isinstance(name, str):\n        name = name.lower()\n        return lambda c: register_class(c, name)\n\n    cls = name\n    name = cls.__name__\n    register_class(cls, name.lower())\n\n    return cls\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "data", "method_configs.py"], "context_start_lineno": 0, "lineno": 14, "function_name": "register_method"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def register_class(cls, name):\n        _METHODS[name] = cls\n        setattr(sys.modules[__name__], name, cls)\n        return cls\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "data", "method_configs.py"], "context_start_lineno": 0, "lineno": 15, "function_name": "register_class"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def get_method(name: str) -> MethodConfig:\n    \"\"\"\n    Return constructor for specified method config\n    \"\"\"\n    name = name.lower()\n    if name in _METHODS:\n        return _METHODS[name]\n    else:\n        raise Exception(\"Error: Trying to access a method that has not been registered\")\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "data", "method_configs.py"], "context_start_lineno": 0, "lineno": 51, "function_name": "get_method"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def register_trainer(name):\n    \"\"\"Decorator used to register a trainer\n    Args:\n        name: Name of the trainer type to register\n    \"\"\"\n    def register_class(cls, name):\n        _TRAINERS[name] = cls\n        setattr(sys.modules[__name__], name, cls)\n        return cls\n\n    if isinstance(name, str):\n        name = name.lower()\n        return lambda c: register_class(c, name)\n\n    cls = name\n    name = cls.__name__\n    register_class(cls, name.lower())\n\n    return cls\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "trainer", "__init__.py"], "context_start_lineno": 0, "lineno": 17, "function_name": "register_trainer"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def register_class(cls, name):\n        _TRAINERS[name] = cls\n        setattr(sys.modules[__name__], name, cls)\n        return cls\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "trainer", "__init__.py"], "context_start_lineno": 0, "lineno": 18, "function_name": "register_class"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def _trainer_unavailble(name):\n        def log_error(*args, **kwargs):\n            raise ImportError(f\"Unable to import NeMo so {name} is unavailable\")\n\n        return register_trainer(name)(log_error)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "loading.py"], "context_start_lineno": 0, "lineno": 17, "function_name": "_trainer_unavailble"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def ault_log_level`\n    \"\"\"\n    env_level_str = os.getenv(\"TRLX_VERBOSITY\", None)\n    if env_level_str:\n        if env_level_str.lower() in log_levels:\n            return log_levels[env_level_str.lower()]\n        else:\n            logging.getLogger().warning(\n                f\"Unknown option TRLX_VERBOSITY={env_level_str}, \" f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n            )\n    return _default_log_level\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "logging.py"], "context_start_lineno": 0, "lineno": 51, "function_name": "_get_default_logging_level"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def _configure_library_root_logger() -> None:\n    global _default_handler\n\n    with _lock:\n        if _default_handler:\n            # This library has already configured the library root logger.\n            return\n        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n        _default_handler.flush = sys.stderr.flush\n\n        # Apply our default configuration to the library root logger.\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.addHandler(_default_handler)\n        library_root_logger.setLevel(_get_default_logging_level())\n        library_root_logger.propagate = False\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "logging.py"], "context_start_lineno": 0, "lineno": 71, "function_name": "_configure_library_root_logger"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def get_scheduler_class(name: SchedulerName):\n    \"\"\"\n    Returns the scheduler class with the given name\n    \"\"\"\n    if name == SchedulerName.COSINE_ANNEALING:\n        return CosineAnnealingLR\n    if name == SchedulerName.LINEAR:\n        return LinearLR\n    supported_schedulers = [s.value for s in SchedulerName]\n    raise ValueError(f\"`{name}` is not a supported scheduler. \" f\"Supported schedulers are: {supported_schedulers}\")\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "__init__.py"], "context_start_lineno": 0, "lineno": 134, "function_name": "get_scheduler_class"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def make_head(n_embd: int, out: int, dtype: type = torch.float32) -> nn.Sequential:\n    \"\"\"Returns a generic sequential MLP head.\"\"\"\n    return nn.Sequential(\n        nn.Linear(n_embd, n_embd * 2, dtype=dtype),\n        nn.ReLU(),\n        nn.Linear(n_embd * 2, out, dtype=dtype),\n    )\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 0, "lineno": 26, "function_name": "make_head"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def rhasattr(obj, attr):\n    \"\"\"A chain-able attribute version of hasattr. For example, to check if\n    `obj` has the attribute `foo.bar.baz`, you can use:\n        `rhasattr(obj, \"foo.bar.baz\")`\n    Reference: https://stackoverflow.com/a/67303315\n    \"\"\"\n    _nested_attrs = attr.split(\".\")\n    _curr_obj = obj\n    for _a in _nested_attrs[:-1]:\n        if hasattr(_curr_obj, _a):\n            _curr_obj = getattr(_curr_obj, _a)\n        else:\n            return False\n    return hasattr(_curr_obj, _nested_attrs[-1])\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 0, "lineno": 74, "function_name": "rhasattr"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def rgetattr(obj, attr: str, *args) -> object:\n    \"\"\"A chain-able attribute version of getattr. For example, to get the\n    attribute `foo.bar.baz` from `obj`, you can use:\n        `rgetattr(obj, \"foo.bar.baz\")`\n    Reference: https://stackoverflow.com/a/31174427\n    \"\"\"\n    def _getattr(obj, attr):\n        return getattr(obj, attr, *args)\n\n    return functools.reduce(_getattr, [obj] + attr.split(\".\"))\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 0, "lineno": 91, "function_name": "rgetattr"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def findattr(obj, attrs: Tuple[str]) -> Union[object, None]:\n    for attr in attrs:\n        if rhasattr(obj, attr):\n            return rgetattr(obj, attr)\n    raise ValueError(f\"Could not find an attribute from `{attrs}` in `{obj}`\")\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "findattr"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def hf_get_decoder_final_norm(model: nn.Module) -> float:\n    \"\"\"Returns the final (layer) norm of the specified decoder.\n    NOTE: Different model configurations have different final norm attribute names.\n        - transformer.ln_f: (GPT2LMHeadModel, GPTJForCausalLM)\n        - model.decoder.final_layer_norm: (OPTForCausalLM)\n        - gpt_neox.layers.final_layer_norm: (GPTNeoXForCausalLM)\n    \"\"\"\n    norm_attrs = (\n        \"transformer.ln_f\",\n        \"model.decoder.final_layer_norm\",\n        \"decoder.final_layer_norm\",\n        \"gpt_neox.final_layer_norm\",\n    )\n    return findattr(model, norm_attrs)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 0, "lineno": 124, "function_name": "hf_get_decoder_final_norm"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def hf_get_decoder_blocks(model: nn.Module) -> Tuple[nn.Module]:\n    \"\"\"Returns the decoder hidden layers of the specified model.\n    NOTE: Different model configurations have different hidden layer attribute names.\n        - transformer.h: (BloomForCausalLM, GPT2LMHeadModel, GPTJForCausalLM)\n        - model.decoder.layers: (OPTForCausalLM)\n        - gpt_neox.layers: (GPTNeoXForCausalLM)\n        - decoder.block: (T5ForConditionalGeneration)\n    \"\"\"\n    hidden_layers_attrs = (\n        \"h\",\n        \"layers\",\n        \"decoder.layers\",\n        \"transformer.h\",\n        \"model.decoder.layers\",\n        \"gpt_neox.layers\",\n        \"decoder.block\",\n    )\n    return findattr(model, hidden_layers_attrs)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 0, "lineno": 141, "function_name": "hf_get_decoder_blocks"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(self):\n        \"\"\"\n        Calculates the running mean and standard deviation of a data stream. Modified version of\n        https://github.com/DLR-RM/stable-baselines3/blob/a6f5049a99a4c21a6f0bcce458ca3306cef310e0/stable_baselines3/common/running_mean_std.py\n        \"\"\"\n        self.mean = 0\n        self.std = 1\n        self.var = 1\n        self.count = 1e-24\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 88, "lineno": 256, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def update(self, xs: torch.Tensor) -> Tuple[float, float]:\n        \"\"\"Updates running moments from batch's moments computed across ranks\"\"\"\n        if dist.is_initialized():\n            xs_mean, xs_var, xs_count = get_global_statistics(xs)\n        else:\n            xs_count = xs.numel()\n            xs_var, xs_mean = torch.var_mean(xs, unbiased=False)\n\n        delta = xs_mean - self.mean\n        tot_count = self.count + xs_count\n\n        new_sum = xs_var * xs_count\n        # correct old_sum deviation accounting for the new mean\n        old_sum = self.var * self.count + delta**2 * self.count * xs_count / tot_count\n        tot_sum = old_sum + new_sum\n\n        self.mean += delta * xs_count / tot_count\n        self.var = tot_sum / tot_count\n        self.std = (self.var * tot_count / (tot_count - 1)).sqrt()\n        self.count = tot_count\n\n        return xs_mean, (xs_var * xs_count / (xs_count - 1)).sqrt()\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 97, "lineno": 263, "function_name": "update"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def generate_layer_regex(config: transformers.PretrainedConfig, num_layers_unfrozen: int = -1) -> str:\n    \"\"\"Generates a regex range for the specified number of learnable layers.\"\"\"\n    if num_layers_unfrozen == -1:\n        return \"(\\d)+.\"\n    num_hidden_layers = hf_get_num_hidden_layers(config)\n    start_layer = num_hidden_layers - num_layers_unfrozen\n    if start_layer < 0:\n        raise Exception(\"Number of layers unfrozen cannot be greater than number of layers in the model\")\n    pattern = f\"(?:{regex_for_range(start_layer, num_hidden_layers - 1)}).\"\n    return f\"{pattern}\"\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 196, "lineno": 373, "function_name": "generate_layer_regex"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def get_delta_modified_modules(\n    config: transformers.PretrainedConfig,\n    modified_modules: List[str],\n    num_layers_unfrozen: int = -1,\n) -> List[str]:\n    \"\"\"Returns a list of module names to be modified for a given delta method with\n    the specified number of learnable layers.\"\"\"\n    unfrozen_layers_pattern = generate_layer_regex(config, num_layers_unfrozen)\n\n    # [r] for regex as per https://github.com/thunlp/OpenDelta/blob/main/opendelta/utils/name_based_addressing.py#L20\n    regex_prefix = \"[r]\"\n    # TODO (jon-tow): `decoder.block.` is hardcoded to support T5 layer naming.\n    decoder_prefix = \"decoder.block.\" if config.is_encoder_decoder else \"\"\n    module_list = [regex_prefix + decoder_prefix + unfrozen_layers_pattern + module for module in modified_modules]\n    return module_list\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 219, "lineno": 390, "function_name": "get_delta_modified_modules"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "default mapping\n    # 2b) Convert the list of `modified_modules` to a range of layers that fit within the range\n    #    of learnable layers as specified by `num_layers_unfrozen`\n\n    # Pop `delta_type` to allow passing the kwargs to the model constructor since\n    # `delta_type` is not a valid argument of the constructor\n    delta_type = delta_kwargs.pop(\"delta_type\")\n    assert delta_type in [\"lora\"], \"Only `LoRA` based delta models are supported\"\n\n    # Use `trlx` default modified modules if none are specified\n    modified_modules = delta_kwargs.get(\"modified_modules\", \"all\")\n    if modified_modules in [\"all\", \"attention\", \"mlp\"]:\n        if config.model_type not in MODIFIED_MODULES_DICT:\n            raise ValueError(\n                f\"Model type `{config.model_type}` is not currently supported for \"\n                \"delta training with default modified modules.\"\n            )\n        modified_modules = MODIFIED_MODULES_DICT[config.model_type][modified_modules]\n    # Update the `modified_modules` with the correct layer ranges\n    delta_kwargs[\"modified_modules\"] = get_delta_modified_modules(\n        config, modified_modules, num_layers_unfrozen=num_layers_unfrozen\n    )\n\n    return delta_type, delta_kwargs\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 264, "lineno": 427, "function_name": "parse_delta_kwargs"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def split_to_patterns(min_, max_):\n        subpatterns = []\n        start = min_\n        for stop in split_to_ranges(min_, max_):\n            subpatterns.append(range_to_pattern(start, stop))\n            start = stop + 1\n        return subpatterns\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 298, "lineno": 457, "function_name": "split_to_patterns"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def split_to_ranges(min_, max_):\n        stops = {max_}\n        nines_count = 1\n        stop = fill_by_nines(min_, nines_count)\n        while min_ <= stop < max_:\n            stops.add(stop)\n            nines_count += 1\n            stop = fill_by_nines(min_, nines_count)\n        zeros_count = 1\n        stop = fill_by_zeros(max_ + 1, zeros_count) - 1\n        while min_ < stop <= max_:\n            stops.add(stop)\n            zeros_count += 1\n            stop = fill_by_zeros(max_ + 1, zeros_count) - 1\n        stops = list(stops)\n        stops.sort()\n        return stops\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 305, "lineno": 465, "function_name": "split_to_ranges"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def range_to_pattern(start, stop):\n        pattern = \"\"\n        any_digit_count = 0\n        for start_digit, stop_digit in zip(str(start), str(stop)):\n            if start_digit == stop_digit:\n                pattern += start_digit\n            elif start_digit != \"0\" or stop_digit != \"9\":\n                pattern += \"[{}-{}]\".format(start_digit, stop_digit)\n            else:\n                any_digit_count += 1\n        if any_digit_count:\n            pattern += r\"\\d\"\n        if any_digit_count > 1:\n            pattern += \"{{{}}}\".format(any_digit_count)\n        return pattern\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "utils", "modeling.py"], "context_start_lineno": 330, "lineno": 489, "function_name": "range_to_pattern"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        position_ids: Optional[List[torch.FloatTensor]] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithValue]:\n        forward_kwargs = self.get_compatible_forward_kwargs(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        forward_kwargs[\"output_hidden_states\"] = True\n        forward_kwargs[\"return_dict\"] = True\n\n        outputs = self.base_model(**forward_kwargs)\n        value = self.v_head(outputs.hidden_states[-1]).squeeze(-1)\n\n        if not return_dict:\n            outputs = (outputs.logits,) + outputs[1:] + (value,)\n            return outputs\n\n        return CausalLMOutputWithValue(**outputs, value=value)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 112, "lineno": 278, "function_name": "forward"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def state_dict(self, *args, **kwargs):\n        \"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the value head\n        to the state dictionary of the wrapped model by prepending the key with `v_head.`.\n        \"\"\"\n        base_model_state_dict = self.base_model.state_dict(*args, **kwargs)\n        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)\n        for k, v in v_head_state_dict.items():\n            base_model_state_dict[f\"v_head.{k}\"] = v\n        return base_model_state_dict\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 146, "lineno": 310, "function_name": "state_dict"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def post_init(self, state_dict):\n        \"\"\"\n        Adds the state dictionary of the value head to the state dictionary of the wrapped model\n        by prepending the key with `v_head.`. This function removes the `v_head.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        for k in list(state_dict.keys()):\n            if \"v_head.\" in k:\n                state_dict[k.replace(\"v_head.\", \"\")] = state_dict.pop(k)\n        self.v_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n        gc.collect()  # noqa: E702\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 159, "lineno": 322, "function_name": "post_init"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(\n        self,\n        base_model: transformers.PreTrainedModel,\n        *,\n        num_layers_unfrozen: int = -1,\n    ):\n        super().__init__(base_model)\n        self.num_layers_unfrozen = num_layers_unfrozen\n        if self.num_layers_unfrozen > 0:\n            config = self.base_model.config\n            branch_class = hf_get_branch_class(config)\n            self.frozen_head = branch_class(\n                self.base_model,\n                num_layers_unfrozen=self.num_layers_unfrozen,\n            ).eval()\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 174, "lineno": 340, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(\n        self,\n        base_model: transformers.PreTrainedModel,\n        *,\n        num_layers_unfrozen: int,\n    ):\n        \"\"\"\n        Args:\n            base_model (transformers.PreTrainedModel): The pretrained model to extract upper trunk from\n            num_layers_unfrozen (int): The number of trainable layers\n        \"\"\"\n        super().__init__(base_model.config)\n\n        # The branch is defined by the last `num_layers_unfrozen` layers of the pretrained model\n        decoder_blocks = deepcopy(hf_get_decoder_blocks(base_model))\n        self.decoder_blocks = nn.ModuleList(list(decoder_blocks)[-num_layers_unfrozen:])\n        self.final_norm = deepcopy(hf_get_decoder_final_norm(base_model))\n        self.lm_head = deepcopy(hf_get_lm_head(base_model))\n\n        self.hidden_size = hf_get_hidden_size(self.config)\n        self.model_parallel = False\n        self.device_map = None\n        self.last_device = None\n        self.gradient_checkpointing = False\n\n        # Freeze the entire branch\n        for parameter in self.parameters():\n            parameter.requires_grad_(False)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 243, "lineno": 409, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(\n        self,\n        base_model: transformers.PreTrainedModel,\n        *,\n        num_layers_unfrozen: int = -1,\n    ):\n        super().__init__(base_model)\n        self.num_layers_unfrozen = num_layers_unfrozen\n        if self.num_layers_unfrozen > 0:\n            branch_class = T5Branch  # TODO: Add support for other model branches\n            self.frozen_head = branch_class(\n                self.base_model,\n                num_layers_unfrozen=self.num_layers_unfrozen,\n            ).eval()\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 744, "lineno": 913, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(\n        self,\n        base_model: transformers.PreTrainedModel,\n        *,\n        num_layers_unfrozen: int,\n    ):\n        super().__init__(base_model, num_layers_unfrozen=num_layers_unfrozen)\n        self.dropout = hf_get_decoder(base_model).dropout\n        self.is_decoder = True\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 837, "lineno": 989, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def hf_get_branch_class(\n    config: transformers.PretrainedConfig,\n) -> \"ModelBranch\":\n    \"\"\"Returns the model branch class for the given config.\"\"\"\n    gpt_branch_supported_archs = [\n        \"GPTJForCausalLM\",\n        \"GPT2LMHeadModel\",\n        \"GPTNeoForCausalLM\",\n        \"GPTNeoXForCausalLM\",\n    ]\n    opt_branch_supported_archs = [\"OPTForCausalLM\"]\n    bloom_branch_supported_archs = [\"BloomModel\", \"BloomForCausalLM\"]\n    arch = config.architectures[0]\n    if arch in gpt_branch_supported_archs:\n        return GPTModelBranch\n    elif arch in opt_branch_supported_archs:\n        return OPTModelBranch\n    elif arch in bloom_branch_supported_archs:\n        return BloomModelBranch\n    else:\n        all_supported_archs = sum(\n            [\n                gpt_branch_supported_archs,\n                opt_branch_supported_archs,\n                bloom_branch_supported_archs,\n            ],\n            [],\n        )\n        raise ValueError(\n            f\"Unsupported architecture: `{arch}`. The following architectures are \"\n            f\"available for model branching:\\n{all_supported_archs}\"\n        )\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ppo.py"], "context_start_lineno": 939, "lineno": 1099, "function_name": "hf_get_branch_class"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def topk_mask(xs: torch.FloatTensor, k: int):\n    if k > xs.shape[-1]:\n        return xs\n    mintop = torch.topk(xs, k)[0][:, -1].unsqueeze(-1)\n    return torch.where(xs < mintop, -np.inf * torch.ones_like(xs, dtype=xs.dtype), xs)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 0, "lineno": 28, "function_name": "topk_mask"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(\n        self,\n        hidden_size: int,\n        vocab_size: int,\n        two_qs: bool,\n        alpha: float,\n        dtype: type,\n    ):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.two_qs = two_qs\n        self.alpha = alpha\n        self.v_head = make_head(self.hidden_size, 1, dtype)\n\n        n_qs = 2 if self.two_qs else 1\n        self.q_heads = nn.ModuleList(make_head(self.hidden_size, self.vocab_size, dtype) for _ in range(n_qs))\n        self.target_q_heads = nn.ModuleList(deepcopy(q_head) for q_head in self.q_heads)\n\n        for target_q_head in self.target_q_heads:\n            target_q_head.requires_grad_(False)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 0, "lineno": 139, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def forward(\n        self,\n        hs: torch.Tensor,\n        states_ixs: torch.Tensor = None,\n        actions_ixs: torch.Tensor = None,\n        **kwargs,\n    ):\n        if states_ixs is not None:\n            states_hs = batched_index_select(hs, states_ixs, 1)\n            actions_hs = batched_index_select(hs, actions_ixs, 1)\n        else:\n            states_hs = actions_hs = hs\n\n        qs = tuple(q_head(actions_hs) for q_head in self.q_heads)\n        target_qs = tuple(q_head(actions_hs) for q_head in self.target_q_heads)\n        vs = self.v_head(states_hs)\n\n        return qs, target_qs, vs\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 0, "lineno": 161, "function_name": "forward"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(\n        self,\n        base_model: transformers.PreTrainedModel,\n        *,\n        two_qs: bool = True,\n        alpha: float = 0.99,\n    ):\n        super().__init__(base_model)\n        hidden_size = hf_get_hidden_size(self.base_model.config)\n        vocab_size = self.base_model.config.vocab_size\n        dtype = next(hf_get_lm_head(self.base_model).parameters()).dtype\n        self.two_qs = two_qs\n        self.alpha = alpha\n        self.ilql_heads = ILQLHeads(hidden_size, vocab_size, self.two_qs, self.alpha, dtype=dtype)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 40, "lineno": 212, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def forward(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        past_key_values=None,\n        actions_ixs=None,\n        states_ixs=None,\n    ):\n        forward_kwargs = self.get_compatible_forward_kwargs(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n        )\n        forward_kwargs[\"output_hidden_states\"] = True\n\n        outputs = self.base_model(**forward_kwargs)\n        qs, target_qs, vs = self.ilql_heads(outputs.hidden_states[-1], states_ixs=states_ixs, actions_ixs=actions_ixs)\n\n        return outputs.logits, qs, target_qs, vs, outputs.past_key_values\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 62, "lineno": 229, "function_name": "forward"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def state_dict(self, *args, **kwargs):\n        \"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the ilql heads\n        to the state dictionary of the wrapped model by prepending the key with `ilql_heads.`.\n        \"\"\"\n        base_model_state_dict = self.base_model.state_dict(*args, **kwargs)\n        ilql_heads_state_dict = self.ilql_heads.state_dict(*args, **kwargs)\n        for k, v in ilql_heads_state_dict.items():\n            base_model_state_dict[f\"ilql_heads.{k}\"] = v\n        return base_model_state_dict\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 155, "lineno": 323, "function_name": "state_dict"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def post_init(self, state_dict):\n        \"\"\"\n        We add the state dictionary of the ilql heads to the state dictionary of the wrapped model\n        by preprending the key with `ilql_heads.`. This function removes the `ilql_heads.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        for k in list(state_dict.keys()):\n            if \"ilql_heads.\" in k:\n                state_dict[k.replace(\"ilql_heads.\", \"\")] = state_dict.pop(k)\n        self.ilql_heads.load_state_dict(state_dict, strict=False)\n        del state_dict\n        gc.collect()\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_ilql.py"], "context_start_lineno": 169, "lineno": 335, "function_name": "post_init"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def __init__(self, base_model: Optional[transformers.PreTrainedModel] = None, **kwargs):\n        super().__init__()\n        self.base_model = base_model\n        # cache `forward` args for general use (avoids incompatible args across architectures)\n        self.forward_kwargs = inspect.getfullargspec(self.base_model.forward).args\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_base.py"], "context_start_lineno": 0, "lineno": 55, "function_name": "__init__"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def _split_kwargs(cls, kwargs: Dict[str, Any]):\n        \"\"\"Separates the kwargs from the supported arguments within `supported_args`\n        and those that are not\n        \"\"\"\n        supported_kwargs = {}\n        unsupported_kwargs = {}\n        for key, value in kwargs.items():\n            if key in cls._supported_args:\n                supported_kwargs[key] = value\n            else:\n                unsupported_kwargs[key] = value\n        return supported_kwargs, unsupported_kwargs\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_base.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "_split_kwargs"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def from_config(cls, config: transformers.PretrainedConfig, **kwargs):\n        \"\"\"Instantiate the pretrained pytorch model from a configuration.\n\n        Args:\n            config (transformers.PretrainedConfig): The configuration to use to\n                instantiate the base model.\n\n        NOTE: Loading a model from its configuration file does **not** load the\n        model weights. It only affects the model's configuration. Use\n        `~transformers.AutoModel.from_pretrained` to load the model weights.\n        \"\"\"\n        if kwargs is not None:\n            wrapped_model_kwargs, from_config_kwargs = cls._split_kwargs(kwargs)\n        else:\n            from_config_kwargs = {}\n            wrapped_model_kwargs = {}\n        base_model = cls._auto_model_parent_class.from_config(config, **from_config_kwargs)\n        model = cls(base_model, **wrapped_model_kwargs)\n        return model\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_base.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "from_config"}}
{"metadata": {"task_id": "CarperAI--trlx/idx", "ground_truth": "def save_pretrained(self, *args, **kwargs):\n        \"\"\"Save the pretrained model to a directory. This method is a wrapper\n        around `transformers.PreTrainedModel.save_pretrained`. Please refer to\n        the documentation of `transformers.PreTrainedModel.save_pretrained` for\n        more information.\n\n        Args:\n            *args (`list`, *optional*):\n                Positional arguments passed along to the underlying model's\n                `save_pretrained` method.\n            **kwargs (`dict`, *optional*):\n                Keyword arguments passed along to the underlying model's\n                `save_pretrained` method.\n        \"\"\"\n        state_dict = kwargs.pop(\"state_dict\", None)\n        if state_dict is None:\n            state_dict = self.state_dict()\n            kwargs[\"state_dict\"] = state_dict\n\n        return self.base_model.save_pretrained(*args, **kwargs)\n", "fpath_tuple": ["CarperAI_trlx", "trlx", "models", "modeling_base.py"], "context_start_lineno": 38, "lineno": 198, "function_name": "save_pretrained"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "first"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "maybe"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "inner"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 54, "function_name": "once"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 57, "function_name": "inner"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "default"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 72, "function_name": "cast_tuple"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "cast_uint8_images_to_float"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 102, "function_name": "zero_init_"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "pad_tuple_to_length"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 139, "function_name": "right_pad_dims_to"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 145, "function_name": "masked_mean"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 188, "function_name": "safe_get_tuple_index"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 204, "function_name": "prob_mask_like"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1, "lineno": 227, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 94, "lineno": 275, "function_name": "q_sample"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 190, "lineno": 326, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 198, "lineno": 333, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 226, "lineno": 381, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 279, "lineno": 453, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 296, "lineno": 475, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 313, "lineno": 505, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 415, "lineno": 602, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 426, "lineno": 615, "function_name": "init_conv_"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 441, "lineno": 629, "function_name": "Downsample"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 471, "lineno": 652, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 476, "lineno": 658, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 484, "lineno": 672, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 489, "lineno": 678, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 514, "lineno": 701, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x, time_emb = None, cond = None):\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 536, "lineno": 732, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 569, "lineno": 765, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 778, "lineno": 949, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 789, "lineno": 961, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 798, "lineno": 968, "function_name": "FeedForward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 824, "lineno": 998, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 832, "lineno": 1008, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 867, "lineno": 1054, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 897, "lineno": 1082, "function_name": "__init__"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 907, "lineno": 1096, "function_name": "forward"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1309, "lineno": 1452, "function_name": "cast_model_parameters"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1815, "lineno": 1985, "function_name": "get_unet"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "check_image_sizes"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "create"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "cycle"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "once"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "get_encoded_dim"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 42, "function_name": "default"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "cast_tuple"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 63, "function_name": "group_dict_by_key"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "groupby_prefix_and_trim"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 82, "function_name": "num_to_groups"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "inner"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 161, "function_name": "find_first"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 226, "lineno": 417, "function_name": "prepare"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 260, "lineno": 455, "function_name": "validate_and_set_unet_being_trained"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 272, "lineno": 469, "function_name": "wrap_unet"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 299, "lineno": 494, "function_name": "set_accelerator_scaler"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 317, "lineno": 515, "function_name": "validate_unet_number"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 328, "lineno": 524, "function_name": "num_steps_taken"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 342, "lineno": 545, "function_name": "add_train_dataloader"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 363, "lineno": 561, "function_name": "add_train_dataset"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 400, "lineno": 592, "function_name": "create_train_iter"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 417, "lineno": 608, "function_name": "train_step"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 442, "lineno": 627, "function_name": "step_with_dl_iter"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 628, "lineno": 819, "function_name": "get_ema_unet"}}
{"metadata": {"task_id": "lucidrains--imagen-pytorch/idx", "ground_truth": "def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n\n            total_loss += loss.item()\n\n            if self.training:\n                self.accelerator.backward(loss)\n\n        return total_loss\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 779, "lineno": 972, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def evaluate(\n      self, expr: rasp.RASPExpr, xs: Sequence[rasp.Value]\n  ) -> Union[Sequence[rasp.Value], rasp.SelectorValue]:\n    out = super().evaluate(expr, xs)\n\n    if not isinstance(expr, rasp.Selector):\n      return out\n\n    out = np.array(out)\n    causal_mask = np.tril(np.full(out.shape, 1))\n    return np.logical_and(causal_mask, out).tolist()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "causal_eval.py"], "context_start_lineno": 0, "lineno": 28, "function_name": "evaluate"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 0, "lineno": 87, "function_name": "__getitem__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 0, "lineno": 158, "function_name": "annotate"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 0, "lineno": 203, "function_name": "__add__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 8, "lineno": 209, "function_name": "__radd__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 14, "lineno": 215, "function_name": "__sub__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 22, "lineno": 221, "function_name": "__rsub__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 27, "lineno": 227, "function_name": "__mul__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 32, "lineno": 233, "function_name": "__rmul__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 46, "lineno": 239, "function_name": "__truediv__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 62, "lineno": 254, "function_name": "__and__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 71, "lineno": 260, "function_name": "__or__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 80, "lineno": 266, "function_name": "__rand__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 88, "lineno": 272, "function_name": "__ror__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 131, "lineno": 334, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 156, "lineno": 359, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 179, "lineno": 381, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 201, "lineno": 406, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __call__(self, key: Value, query: Value) -> bool:\n    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 217, "lineno": 436, "function_name": "__call__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):\n    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 256, "lineno": 487, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, selector: Selector):\n    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 274, "lineno": 516, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 292, "lineno": 529, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self, inner: Selector):\n    self.inner = inner\n    super().__init__()\n    assert isinstance(self.inner, Selector)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 333, "lineno": 559, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def selector_and(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorAnd, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l and r)\n    if simplified:\n      return simplified\n\n  return SelectorAnd(fst, snd)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 355, "lineno": 586, "function_name": "selector_and"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 459, "lineno": 677, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 509, "lineno": 724, "function_name": "default_encoding"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 548, "lineno": 762, "function_name": "default_name"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 584, "lineno": 792, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 626, "lineno": 831, "function_name": "eval_sequence_map"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 643, "lineno": 839, "function_name": "eval_map"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 649, "lineno": 849, "function_name": "eval_constant_sop"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_aggregate(self, sop: Aggregate,\n                     xs: Sequence[Value]) -> Sequence[Value]:\n    selector_value = self.evaluate(sop.selector, xs)\n    values = self.evaluate(sop.sop, xs)\n    default = sop.default\n\n    return [\n        _mean(_get_selected(row, values), default) for row in selector_value\n    ]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 658, "lineno": 861, "function_name": "eval_aggregate"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_select(self, sel: Select, xs: Sequence[Value]) -> SelectorValue:\n    \"\"\"Evaluates a Select on `xs`.\"\"\"\n    key_values = self.evaluate(sel.keys, xs)\n    query_values = self.evaluate(sel.queries, xs)\n\n    key_len = len(key_values)\n    query_len = len(query_values)\n    out = np.zeros((query_len, key_len), dtype=bool).tolist()\n    for row, query in enumerate(query_values):\n      for col, key in enumerate(key_values):\n        out[row][col] = bool(sel.predicate(key, query))\n    return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 673, "lineno": 871, "function_name": "eval_select"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_constant_selector(self, sel: ConstantSelector,\n                             xs: Sequence[Value]) -> SelectorValue:\n    if sel.check_length and (len(xs) != len(sel.value)):\n      raise ValueError(\n          f\"Constant len {len(xs)} doesn't match input len {len(sel.value)}.\")\n    return sel.value\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 691, "lineno": 884, "function_name": "eval_constant_selector"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def eval_selector_and(self, sel: SelectorAnd,\n                        xs: Sequence[Value]) -> SelectorValue:\n    fst_values = self.evaluate(sel.fst, xs)\n    snd_values = self.evaluate(sel.snd, xs)\n    return np.logical_and(np.array(fst_values), np.array(snd_values)).tolist()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 704, "lineno": 891, "function_name": "eval_selector_and"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "default: VT) -> VT:\n  \"\"\"Takes the mean for numbers and concats for strings.\"\"\"\n  if not xs:\n    return default\n  exemplar = xs[0]\n  if isinstance(exemplar, (int, bool)):\n    return sum(xs) / len(xs)\n  elif len(xs) == 1:\n    return exemplar\n  else:\n    raise ValueError(f\"Unsupported type for aggregation: {xs}\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 743, "lineno": 917, "function_name": "_mean"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def has_prev(seq: rasp.SOp) -> rasp.SOp:\n      prev_copy = rasp.SelectorAnd(\n          rasp.Select(seq, seq, rasp.Comparison.EQ),\n          rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LT),\n      )\n      return rasp.Aggregate(prev_copy, rasp.Full(1), default=0) > 0\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp_test.py"], "context_start_lineno": 386, "lineno": 556, "function_name": "has_prev"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def apply(self, tokens: List[bases.Value]) -> AssembledTransformerModelOutput:\n    \"\"\"Returns output from running the model on a set of input tokens.\"\"\"\n    if self.input_encoder:\n      tokens = self.input_encoder.encode(tokens)\n    tokens = jnp.array([tokens])\n    output = self.forward(self.params, tokens)\n    decoded = output.unembedded_output[0].tolist()\n    if self.output_encoder:\n      decoded = self.output_encoder.decode(decoded)\n\n    if self.input_encoder.bos_token:\n      # Special case for decoding the bos token position, for which the output\n      # decoder might have unspecified behavior.\n      decoded = [self.input_encoder.bos_token] + decoded[1:]\n\n    return AssembledTransformerModelOutput(\n        decoded=decoded,\n        unembedded=output.unembedded_output,\n        layer_outputs=output.transformer_output.layer_outputs,\n        residuals=output.transformer_output.residuals,\n        attn_logits=output.transformer_output.attn_logits,\n        transformer_output=output.transformer_output.output,\n        input_embeddings=output.transformer_output.input_embeddings)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "apply"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def unembed(x, use_unembed_argmax):\n    out = x @ res_to_out.matrix\n    if use_unembed_argmax:\n      return jnp.argmax(out, axis=-1)\n    elif out.shape[-1] == 1:\n      return out.squeeze(-1)\n    return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble.py"], "context_start_lineno": 33, "lineno": 197, "function_name": "unembed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def get_compiled_model():\n    transformer = model.Transformer(model_config)\n    embed_modules = _make_embedding_modules(\n        residual_space=residual_space,\n        tokens_space=tokens_space,\n        indices_space=indices_space,\n        output_space=output_space)\n    return model.CompiledTransformerModel(\n        transformer=transformer,\n        token_embed=embed_modules.token_embed,\n        position_embed=embed_modules.pos_embed,\n        unembed=embed_modules.unembed,\n        use_unembed_argmax=categorical_output)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble.py"], "context_start_lineno": 81, "lineno": 247, "function_name": "get_compiled_model"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def assertSequenceEqualWhenExpectedIsNotNone(self, actual_seq, expected_seq):\n    for actual, expected in zip(actual_seq, expected_seq):\n      if expected is not None and actual != expected:\n        self.fail(f\"{actual_seq} does not match (ignoring Nones) \"\n                  f\"expected_seq={expected_seq}\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_transformer_integration_test.py"], "context_start_lineno": 0, "lineno": 37, "function_name": "assertSequenceEqualWhenExpectedIsNotNone"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _check_block_types_are_correct(self, graph):\n    for _, node in graph.nodes.items():\n      expr = node[nodes.EXPR]\n      if isinstance(expr, rasp.SOp):\n        block = node[nodes.MODEL_BLOCK]\n        if isinstance(expr, (rasp.Map, rasp.SequenceMap)):\n          self.assertIsInstance(block, transformers.MLP)\n        elif isinstance(expr, rasp.Aggregate):\n          self.assertIsInstance(block, transformers.AttentionHead)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph_test.py"], "context_start_lineno": 0, "lineno": 31, "function_name": "_check_block_types_are_correct"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _get_input_space_from_node(self, node):\n    block = node[nodes.MODEL_BLOCK]\n    if isinstance(block, transformers.MLP):\n      return block.fst.input_space\n    elif isinstance(block, transformers.AttentionHead):\n      return bases.join_vector_spaces(block.w_qk.left_space,\n                                      block.w_qk.right_space,\n                                      block.w_ov.input_space)\n    else:\n      return None\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph_test.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "_get_input_space_from_node"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _check_spaces_are_consistent(self, graph):\n    \"\"\"Check that for each edge the output is a subspace of the input.\"\"\"\n    for u, v in graph.edges:\n      u_node, v_node = graph.nodes[u], graph.nodes[v]\n      if isinstance(u_node[nodes.EXPR], rasp.SOp) and isinstance(\n          v_node[nodes.EXPR], rasp.SOp):\n        u_out_basis = u_node[nodes.OUTPUT_BASIS]\n        u_out_space = bases.VectorSpaceWithBasis(u_out_basis)\n        v_in_space = self._get_input_space_from_node(v_node)\n        self.assertTrue(u_out_space.issubspace(v_in_space))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph_test.py"], "context_start_lineno": 0, "lineno": 53, "function_name": "_check_spaces_are_consistent"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"\n  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n\n  result = -1\n  for source in sources:\n    all_paths = nx.all_simple_paths(graph, source[nodes.ID], node[nodes.ID])\n    longest_path_len = max(map(num_sops, all_paths), default=-1) - 1\n    if longest_path_len > result:\n      result = longest_path_len\n  return result\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 44, "function_name": "_get_longest_path_length_to_node"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "num_sops"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _node_is_attn(node: Node) -> bool:\n  \"\"\"Returns True if node is an attention layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(\n      node[nodes.MODEL_BLOCK],\n      (transformers.AttentionHead, transformers.MultiAttentionHead))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "_node_is_attn"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _node_is_residual_block(node: Node) -> bool:\n  \"\"\"Returns True if node is a valid residual block (Attn followed by MLP).\"\"\"\n  block = node[nodes.MODEL_BLOCK] if nodes.MODEL_BLOCK in node else None\n  if block and isinstance(block, transformers.SeriesWithResiduals):\n    if len(block.blocks) == 2:\n      attn, mlp = block.blocks\n      if (isinstance(\n          attn,\n          (transformers.AttentionHead, transformers.MultiAttentionHead)) and\n          isinstance(mlp, transformers.MLP)):\n        return True\n  return False\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 78, "function_name": "_node_is_residual_block"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _all_attn_nodes(node_list: Sequence[Node]) -> bool:\n  \"\"\"Returns True iff all nodes are attention layers (or nodes is empty).\"\"\"\n  for node in node_list:\n    if not _node_is_attn(node):\n      return False\n  return True\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 92, "function_name": "_all_attn_nodes"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _all_mlp_nodes(node_list: Sequence[Node]) -> bool:\n  \"\"\"Returns True iff all nodes are MLP layers (or nodes is empty).\"\"\"\n  for node in node_list:\n    if not _node_is_mlp(node):\n      return False\n  return True\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 100, "function_name": "_all_mlp_nodes"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _transform_fun_to_basis_fun(\n    fun: Callable[..., Any],\n    output_direction_name: Optional[str] = None) -> Callable[..., Any]:\n  \"\"\"Transforms a function acting on values into one acting on directions.\"\"\"\n  def bases_fun(*args):\n    values = [d.value for d in args]\n    result = fun(*values)\n    if output_direction_name:\n      return bases.BasisDirection(output_direction_name, result)\n    return result\n\n  return bases_fun\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph.py"], "context_start_lineno": 0, "lineno": 33, "function_name": "_transform_fun_to_basis_fun"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def bases_fun(*args):\n    values = [d.value for d in args]\n    result = fun(*values)\n    if output_direction_name:\n      return bases.BasisDirection(output_direction_name, result)\n    return result\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph.py"], "context_start_lineno": 0, "lineno": 34, "function_name": "bases_fun"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _get_dummy_block(self, block_type):\n    if block_type == \"ATTN\":\n      return categorical_attn.categorical_attn(\n          query_space=bases.VectorSpaceWithBasis.from_names([\"query\"]),\n          key_space=bases.VectorSpaceWithBasis.from_names([\"bos\", \"key\"]),\n          value_space=bases.VectorSpaceWithBasis.from_names([\"bos\", \"value\"]),\n          output_space=bases.VectorSpaceWithBasis.from_names([\"output\"]),\n          bos_space=bases.VectorSpaceWithBasis.from_names([\"bos\"]),\n          one_space=bases.VectorSpaceWithBasis.from_names([\"one\"]),\n          attn_fn=lambda x, y: True,\n      )\n    elif block_type == \"MLP\":\n      return categorical_mlp.map_categorical_mlp(\n          input_space=bases.VectorSpaceWithBasis.from_names([\"input\"]),\n          output_space=bases.VectorSpaceWithBasis.from_names([\"output\"]),\n          operation=lambda x: x,\n      )\n    else:\n      return None\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model_test.py"], "context_start_lineno": 0, "lineno": 31, "function_name": "_get_dummy_block"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def ensure_node(expr: rasp.RASPExpr) -> NodeID:\n    \"\"\"Finds or creates a graph node corresponding to expr; returns its ID.\"\"\"\n    node_id = expr.label\n    if node_id not in graph:\n      graph.add_node(node_id, **{nodes.ID: node_id, nodes.EXPR: expr})\n\n    return node_id\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_graph.py"], "context_start_lineno": 0, "lineno": 43, "function_name": "ensure_node"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def visit_raspexpr(expr: rasp.RASPExpr):\n    parent_id = ensure_node(expr)\n\n    for child_expr in expr.children:\n      expr_queue.put(child_expr)\n      child_id = ensure_node(child_expr)\n      graph.add_edge(child_id, parent_id)\n\n    if not expr.children:\n      sources.append(graph.nodes[parent_id])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_graph.py"], "context_start_lineno": 0, "lineno": 52, "function_name": "visit_raspexpr"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _make_input_space(vocab, max_seq_len):\n  tokens_space = bases.VectorSpaceWithBasis.from_values(\"tokens\", vocab)\n  indices_space = bases.VectorSpaceWithBasis.from_values(\n      \"indices\", range(max_seq_len))\n  one_space = bases.VectorSpaceWithBasis.from_names([_ONE_DIRECTION])\n  bos_space = bases.VectorSpaceWithBasis.from_names([_BOS_DIRECTION])\n  input_space = bases.join_vector_spaces(tokens_space, indices_space, one_space,\n                                         bos_space)\n\n  return input_space\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_craft_integration_test.py"], "context_start_lineno": 0, "lineno": 36, "function_name": "_make_input_space"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _embed_input(input_seq, input_space):\n  bos_vec = input_space.vector_from_basis_direction(\n      bases.BasisDirection(_BOS_DIRECTION))\n  one_vec = input_space.vector_from_basis_direction(\n      bases.BasisDirection(_ONE_DIRECTION))\n  embedded_input = [bos_vec + one_vec]\n  for i, val in enumerate(input_seq):\n    i_vec = input_space.vector_from_basis_direction(\n        bases.BasisDirection(\"indices\", i))\n    val_vec = input_space.vector_from_basis_direction(\n        bases.BasisDirection(\"tokens\", val))\n    embedded_input.append(i_vec + val_vec + one_vec)\n  return bases.VectorInBasis.stack(embedded_input)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_craft_integration_test.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "_embed_input"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _embed_output(output_seq, output_space, categorical_output):\n  embedded_output = []\n  output_label = output_space.basis[0].name\n  for x in output_seq:\n    if x is None:\n      out_vec = output_space.null_vector()\n    elif categorical_output:\n      out_vec = output_space.vector_from_basis_direction(\n          bases.BasisDirection(output_label, x))\n    else:\n      out_vec = x * output_space.vector_from_basis_direction(\n          output_space.basis[0])\n    embedded_output.append(out_vec)\n  return bases.VectorInBasis.stack(embedded_output)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_craft_integration_test.py"], "context_start_lineno": 0, "lineno": 63, "function_name": "_embed_output"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "defg\")\n    >> [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n\n  Returns:\n    length: SOp mapping an input to a sequence, where every element\n      is the length of that sequence.\n  \"\"\"\n  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 35, "function_name": "make_length"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_reverse(sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Create an SOp that reverses a sequence, using length primitive.\n\n  Example usage:\n    reverse = make_reverse(rasp.tokens)\n    reverse(\"Hello\")\n    >> ['o', 'l', 'l', 'e', 'H']\n\n  Args:\n    sop: an SOp\n\n  Returns:\n    reverse : SOp that reverses the input sequence.\n  \"\"\"\n  opp_idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 57, "function_name": "make_reverse"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_pair_balance(sop: rasp.SOp, open_token: str,\n                      close_token: str) -> rasp.SOp:\n  \"\"\"Return fraction of previous open tokens minus the fraction of close tokens.\n\n   (As implemented in the RASP paper.)\n\n  If the outputs are always non-negative and end in 0, that implies the input\n  has balanced parentheses.\n\n  Example usage:\n    num_l = make_pair_balance(rasp.tokens, \"(\", \")\")\n    num_l(\"a()b(c))\")\n    >> [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"\n  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 87, "function_name": "make_pair_balance"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 165, "function_name": "make_hist"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 184, "function_name": "make_sort_unique"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 34, "lineno": 216, "function_name": "make_sort"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"\n  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 57, "lineno": 235, "function_name": "make_sort_freq"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_frac_prevs(bools: rasp.SOp) -> rasp.SOp:\n  \"\"\"Count the fraction of previous tokens where a specific condition was True.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    num_l = make_frac_prevs(rasp.tokens==\"l\")\n    num_l(\"hello\")\n    >> [0, 0, 1/3, 1/2, 2/5]\n\n  Args:\n    bools: SOp mapping a sequence to a sequence of booleans.\n\n  Returns:\n    frac_prevs: SOp mapping an input to a sequence, where every element\n      is the fraction of previous \"True\" tokens.\n  \"\"\"\n  bools = rasp.numerical(bools)\n  prevs = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LEQ)\n  return rasp.numerical(rasp.Aggregate(prevs, bools,\n                                       default=0)).named(\"frac_prevs\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 76, "lineno": 260, "function_name": "make_frac_prevs"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def shift_by(offset: int, /, sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns the sop, shifted by `offset`, None-padded.\"\"\"\n  select_off_by_offset = rasp.Select(rasp.indices, rasp.indices,\n                                     lambda k, q: q == k + offset)\n  out = rasp.Aggregate(select_off_by_offset, sop, default=None)\n  return out.named(f\"shift_by({offset})\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 88, "lineno": 268, "function_name": "shift_by"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def detect_pattern(sop: rasp.SOp, pattern: Sequence[rasp.Value]) -> rasp.SOp:\n  \"\"\"Returns an SOp which is True at the final element of the pattern.\n\n  The first len(pattern) - 1 elements of the output SOp are None-padded.\n\n  detect_pattern(tokens, \"abc\")(\"abcabc\") == [None, None, T, F, F, T]\n\n  Args:\n    sop: the SOp in which to look for patterns.\n    pattern: a sequence of values to look for.\n\n  Returns:\n    a sop which detects the pattern.\n  \"\"\"\n  if len(pattern) < 1:\n    raise ValueError(f\"Length of `pattern` must be at least 1. Got {pattern}\")\n\n  # detectors[i] will be a boolean-valued SOp which is true at position j iff\n  # the i'th (from the end) element of the pattern was detected at position j-i.\n  detectors = []\n  for i, element in enumerate(reversed(pattern)):\n    detector = sop == element\n    if i != 0:\n      detector = shift_by(i, detector)\n    detectors.append(detector)\n\n  # All that's left is to take the AND over all detectors.\n  pattern_detected = detectors.pop()\n  while detectors:\n    pattern_detected = pattern_detected & detectors.pop()\n\n  return pattern_detected.named(f\"detect_pattern({pattern})\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 106, "lineno": 289, "function_name": "detect_pattern"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_count_less_freq(n: int) -> rasp.SOp:\n  \"\"\"Returns how many tokens appear fewer than n times in the input.\n\n  The output sequence contains this count in each position.\n\n  Example usage:\n    count_less_freq = make_count_less_freq(2)\n    count_less_freq([\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"])\n    >> [3, 3, 3, 3, 3, 3]\n    count_less_freq([\"a\", \"a\", \"c\", \"b\", \"b\", \"c\"])\n    >> [6, 6, 6, 6, 6, 6]\n\n  Args:\n    n: Integer to compare token frequences to.\n  \"\"\"\n  hist = make_hist().named(\"hist\")\n  select_less = rasp.Select(hist, hist,\n                            lambda x, y: x <= n).named(\"select_less\")\n  return rasp.SelectorWidth(select_less).named(\"count_less_freq\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 143, "lineno": 324, "function_name": "make_count_less_freq"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_nary_sequencemap(f, *sops):\n  \"\"\"Returns an SOp that simulates an n-ary SequenceMap.\n\n  Uses multiple binary SequenceMaps to convert n SOps x_1, x_2, ..., x_n\n  into a single SOp arguments that takes n-tuples as value. The n-ary sequence\n  map implementing f is then a Map on this resulting SOp.\n\n  Note that the intermediate variables representing tuples of varying length\n  will be encoded categorically, and can become very high-dimensional. So,\n  using this function might lead to very large compiled models.\n\n  Args:\n    f: Function with n arguments.\n    *sops: Sequence of SOps, one for each argument of f.\n  \"\"\"\n  values, *sops = sops\n  for sop in sops:\n    # x is a single entry in the first iteration but a tuple in later iterations\n    values = rasp.SequenceMap(\n        lambda x, y: (*x, y) if isinstance(x, tuple) else (x, y), values, sop)\n  return rasp.Map(lambda args: f(*args), values)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 184, "lineno": 365, "function_name": "make_nary_sequencemap"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def token_pos_embed(tokens):\n      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.token_embed(tokens)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble_test.py"], "context_start_lineno": 0, "lineno": 40, "function_name": "token_pos_embed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def token_pos_embed(tokens):\n      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.pos_embed(jnp.indices(tokens.shape)[-1])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble_test.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "token_pos_embed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def unembed(embeddings):\n      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.unembed(embeddings, use_unembed_argmax=True)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble_test.py"], "context_start_lineno": 0, "lineno": 96, "function_name": "unembed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "define W_new: V x V ->  st:\n    # W_new(one, bos) = 0.5, otherwise 0.\n    #\n    # Now set W_QK' = W_QK + W_new.\n    #\n    # To evaluate the attention to the BOS position:\n    # W_QK'(q, bos + one)\n    # = W_QK'(q, bos) + W_QK'(q, one)\n    # = W_QK(q, bos) + W_QK(q, one) + W_new(q, bos) + W_new(q, one)\n    # = 0            + 0            + W_new(q, bos) + W_new(q, one)\n    # = W_new(q, bos) + W_new(q, one)\n    # = W_new(q' + one, bos) + W_new(q' + one, one)  where q = one + q'\n    # = W_new(q', bos) + W_new(one, bos) + W_new(q', one) + W_new(one, one)\n    # = 0              + 0.5             + 0              + 0\n    # = 0.5\n    #\n    # To evaluate the attention to a non-BOS position:\n    # W_QK'(0 * bos + q, 0 * bos + k)  # s.t. q  Q+{one}, k  K+{one}\n    # = 0*W_QK'(bos, 0*bos + k) + W_QK'(q, 0*bos + k)\n    # = W_QK'(q, 0*bos + k)\n    # = 0*W_QK'(q, bos) + W_QK'(q, k)\n    # = W_QK'(q, k)\n    # = W_QK(q, k)    since W_QK' = W_QK on inputs not containing bos.\n    # = W_QK(q', k')  since W_QK(x, y) = 0 whenever x or y are one.\n    #\n    # Since W_QK(q, k) takes values in 0, 1, a sufficiently high softmax\n    # coldness will give us the desired property.                            QED\n    #\n    # The following implements this idea.\n    # By replacing 0.5 with 1, we can instead enforce a different property: that\n    # the BOS token is always attended to in addition to whatever else.\n    if key == bos_direction and query == one_direction:\n      c = 1. if always_attend_to_bos else 0.5\n      return c * softmax_coldness\n    elif {key, query}.intersection({one_direction, bos_direction}):\n      return 0\n\n    return softmax_coldness * attn_fn(query, key)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_attn.py"], "context_start_lineno": 0, "lineno": 142, "function_name": "qk_fun"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def ov_fun(input_dir: bases.BasisDirection) -> bases.VectorInBasis:\n    if use_bos_for_default_output and input_dir == bos_direction:\n      return default_output\n    return output_space.vector_from_basis_direction(value_to_output[input_dir])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_attn.py"], "context_start_lineno": 16, "lineno": 157, "function_name": "ov_fun"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def action(direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # hidden_k_0 = ReLU(L * (x - threshold[k]) + 1)\n    # hidden_k_1 = ReLU(L * (x - threshold[k]))\n    if direction == one_direction:\n      hidden = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(f\"{hidden_name}start\"))\n    else:\n      hidden = hidden_space.null_vector()\n    for k in range(1, num_vals):\n      vec0 = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(hidden_name, (k, 0)))\n      vec1 = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(hidden_name, (k, 1)))\n      if direction == one_direction:\n        hidden += (1 - large_number * value_thresholds[k - 1]) * vec0\n        hidden -= large_number * value_thresholds[k - 1] * vec1\n      else:\n        hidden += large_number * vec0 + large_number * vec1\n    return hidden\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # output = sum(\n    #     (hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n    #   for all k)\n    if direction.name == f\"{hidden_name}start\":\n      return discretising_layer.output_values[0] * out_vec\n    k, i = direction.value\n    # add hidden_k_0 and subtract hidden_k_1\n    sign = {0: 1, 1: -1}[i]\n    return sign * (discretising_layer.output_values[k] -\n                   discretising_layer.output_values[k - 1]) * out_vec\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 42, "lineno": 192, "function_name": "second_layer_action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    \"\"\"Computes output value and returns corresponding output direction.\"\"\"\n    if direction.name == f\"{hidden_name}start\":\n      return vec_by_out_val[discretising_layer.output_values[0]]\n    else:\n      k, i = direction.value\n      # add hidden_k_0 and subtract hidden_k_1\n      sign = {0: 1, 1: -1}[i]\n      out_k = discretising_layer.output_values[k]\n      out_k_m_1 = discretising_layer.output_values[k - 1]\n      return sign * (vec_by_out_val[out_k] - vec_by_out_val[out_k_m_1])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 109, "lineno": 261, "function_name": "second_layer_action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def first_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    output = hidden_space.null_vector()\n    if direction == input1_basis_direction:\n      output += x_pos_vec - x_neg_vec\n    if direction == input2_basis_direction:\n      output += y_pos_vec - y_neg_vec\n    return output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 165, "lineno": 312, "function_name": "first_layer_action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    if direction.name == f\"{hidden_name}x\":\n      return input1_factor * direction.value * out_vec\n    if direction.name == f\"{hidden_name}y\":\n      return input2_factor * direction.value * out_vec\n    return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 173, "lineno": 324, "function_name": "second_layer_action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def make_input(x):\n      return (one_vec + x * value_vec +\n              residual_space.vector_from_basis_direction(\n                  bases.BasisDirection(\"input\", x)))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_attn_test.py"], "context_start_lineno": 0, "lineno": 137, "function_name": "make_input"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 43, "function_name": "map_categorical_mlp"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 44, "function_name": "operation_fn"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def map_categorical_to_numerical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.Value], float],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a categorical to a numerical variable.\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: Vector space containing the input x.\n    output_space: Vector space to write the numerical output to.\n    operation: A function operating on basis directions.\n  \"\"\"\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  def operation_fn(direction):\n    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 73, "function_name": "map_categorical_to_numerical_mlp"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def operation_fn(direction):\n    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "operation_fn"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def from_hidden(h):\n    x_name, x_value, y_name, y_value = h.value\n    x_dir = bases.BasisDirection(x_name, x_value)\n    y_dir = bases.BasisDirection(y_name, y_value)\n    return x_dir, y_dir\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 126, "function_name": "from_hidden"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def logical_and(direction):\n    if direction in one_space:\n      out = bases.VectorInBasis(hidden_space.basis,\n                                -np.ones(hidden_space.num_dims))\n    elif direction in input1_space:\n      dir1 = direction\n      out = hidden_space.null_vector()\n      for dir2 in input2_space.basis:\n        out += hidden_space.vector_from_basis_direction(to_hidden(dir1, dir2))\n    else:\n      dir2 = direction\n      out = hidden_space.null_vector()\n      for dir1 in input1_space.basis:\n        out += hidden_space.vector_from_basis_direction(to_hidden(dir1, dir2))\n    return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 138, "function_name": "logical_and"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def operation_fn(direction):\n    dir1, dir2 = from_hidden(direction)\n    output_direction = operation(dir1, dir2)\n    if output_direction in output_space:\n      return output_space.vector_from_basis_direction(output_direction)\n    else:\n      return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 157, "function_name": "operation_fn"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 56, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 66, "function_name": "__call__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 82, "function_name": "from_action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 95, "function_name": "combine_in_parallel"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 101, "function_name": "action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def project(\n    from_space: VectorSpaceWithBasis,\n    to_space: VectorSpaceWithBasis,\n) -> Linear:\n  \"\"\"Creates a projection.\"\"\"\n  def action(direction: bases.BasisDirection) -> VectorInBasis:\n    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n\n  return Linear.from_action(from_space, to_space, action=action)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 117, "function_name": "project"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def action(direction: bases.BasisDirection) -> VectorInBasis:\n    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 118, "function_name": "action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __post_init__(self):\n    \"\"\"Ensure matrix acts in sorted bases and typecheck sizes.\"\"\"\n    left_size, right_size = self.matrix.shape\n    assert left_size == self.left_space.num_dims\n    assert right_size == self.right_space.num_dims\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 135, "function_name": "__post_init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __call__(self, x: VectorInBasis, y: VectorInBasis) -> float:\n    \"\"\"Describes the action of the operator on vectors.\"\"\"\n    if x not in self.left_space:\n      raise TypeError(f\"x={x} not in self.left_space={self.left_space}.\")\n    if y not in self.right_space:\n      raise TypeError(f\"y={y} not in self.right_space={self.right_space}.\")\n    return (x.magnitudes.T @ self.matrix @ y.magnitudes).item()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 141, "function_name": "__call__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def from_action(\n      cls,\n      left_space: VectorSpaceWithBasis,\n      right_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection, BasisDirection], float],\n  ) -> \"ScalarBilinear\":\n    \"\"\"from_action(l, r)(action) creates a ScalarBilinear.\"\"\"\n    matrix = np.zeros((left_space.num_dims, right_space.num_dims))\n    for i, left_direction in enumerate(left_space.basis):\n      for j, right_direction in enumerate(right_space.basis):\n        matrix[i, j] = action(left_direction, right_direction)\n\n    return ScalarBilinear(left_space, right_space, matrix)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "from_action"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "__str__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 46, "function_name": "__lt__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "__post_init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 76, "function_name": "__add__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 87, "function_name": "__sub__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __eq__(self, other: \"VectorInBasis\") -> bool:\n    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 111, "function_name": "__eq__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def stack(cls,\n            vectors: Sequence[\"VectorInBasis\"],\n            axis: int = 0) -> \"VectorInBasis\":\n    for v in vectors[1:]:\n      if v.basis_directions != vectors[0].basis_directions:\n        raise TypeError(f\"Stacking incompatible bases: {vectors[0]} + {v}\")\n    return cls(vectors[0].basis_directions,\n               np.stack([v.magnitudes for v in vectors], axis=axis))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 124, "function_name": "stack"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def project(\n      self, basis: Union[\"VectorSpaceWithBasis\", Sequence[BasisDirection]]\n  ) -> \"VectorInBasis\":\n    \"\"\"Projects to the basis.\"\"\"\n    if isinstance(basis, VectorSpaceWithBasis):\n      basis = basis.basis\n    components = []\n    for direction in basis:\n      if direction in self.basis_directions:\n        components.append(\n            self.magnitudes[..., self.basis_directions.index(direction)])\n      else:\n        components.append(np.zeros_like(self.magnitudes[..., 0]))\n    return VectorInBasis(list(basis), np.stack(components, axis=-1))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 134, "function_name": "project"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __contains__(self, item: Union[VectorInBasis, BasisDirection]) -> bool:\n    if isinstance(item, BasisDirection):\n      return item in self.basis\n\n    return set(self.basis) == set(item.basis_directions)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 160, "function_name": "__contains__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def direct_sum(*vs: VectorSpaceWithBasis) -> VectorSpaceWithBasis:\n  \"\"\"Create a direct sum of the vector spaces.\n\n  Assumes the basis elements of all input vector spaces are\n  orthogonal to each other. Maintains the order of the bases.\n\n  Args:\n    *vs: the vector spaces to sum.\n\n  Returns:\n    the combined vector space.\n\n  Raises:\n    Value error in case of overlapping bases.\n  \"\"\"\n  # Take the union of all the bases:\n  total_basis = sum([v.basis for v in vs], [])\n\n  if len(total_basis) != len(set(total_basis)):\n    raise ValueError(\"Overlapping bases!\")\n\n  return VectorSpaceWithBasis(total_basis)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 43, "lineno": 211, "function_name": "direct_sum"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def join_vector_spaces(*vs: VectorSpaceWithBasis) -> VectorSpaceWithBasis:\n  \"\"\"Joins a set of vector spaces allowing them to overlap.\n\n  Assumes the basis elements of all input vector spaces are\n  orthogonal to each other. Does not maintain the order of the bases but\n  sorts them.\n\n  Args:\n    *vs: the vector spaces to sum.\n\n  Returns:\n    the combined vector space.\n  \"\"\"\n  # Take the union of all the bases:\n  total_basis = list(set().union(*[set(v.basis) for v in vs]))\n  total_basis = sorted(total_basis)\n  return VectorSpaceWithBasis(total_basis)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 61, "lineno": 233, "function_name": "join_vector_spaces"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "__post_init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "apply"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "w_ov_residual"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __post_init__(self):\n    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "__post_init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def heads(self) -> Iterable[AttentionHead]:\n    for sub_block in self.sub_blocks:\n      if isinstance(sub_block, AttentionHead):\n        yield sub_block\n      elif isinstance(sub_block, MultiAttentionHead):\n        yield from sub_block.heads()\n      else:\n        raise NotImplementedError()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 130, "function_name": "heads"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __post_init__(self):\n    \"\"\"Typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.fst.input_space,\n                                                     self.snd.output_space)\n\n    assert self.fst.output_space == self.snd.input_space\n    assert self.fst.input_space.issubspace(self.residual_space)\n    assert self.snd.output_space.issubspace(self.residual_space)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 151, "function_name": "__post_init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n\n    x = project(self.residual_space, self.fst.input_space)(x)\n    hidden = self.fst(x)\n    hidden = relu(hidden)\n    out = self.snd(hidden)\n    return project(self.snd.output_space, self.residual_space)(out)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 160, "function_name": "apply"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def combine_in_parallel(cls, mlps: Sequence[\"MLP\"]) -> \"MLP\":\n    fst = vectorspace_fns.Linear.combine_in_parallel(\n        [block.fst for block in mlps])\n    snd = vectorspace_fns.Linear.combine_in_parallel(\n        [block.snd for block in mlps])\n    return cls(fst=fst, snd=snd, residual_space=None)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 170, "function_name": "combine_in_parallel"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    x = x.project(self.residual_space)\n    for block in self.blocks:\n      x_in = x.project(block.residual_space)\n      x_out = block.apply(x_in).project(self.residual_space)\n      x = x + x_out\n    return x\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 23, "lineno": 191, "function_name": "apply"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def ignoring_arithmetic_errors(fun: Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Makes fun return None instead of raising ArithmeticError.\"\"\"\n\n  @functools.wraps(fun)\n  def fun_wrapped(*args):\n    try:\n      return fun(*args)\n    except ArithmeticError:\n      logging.warning(\n          \"Encountered arithmetic error in function: for value %s. \"\n          \"Assuming this input will never occur.\", str(args))\n      return None\n\n  return fun_wrapped\n", "fpath_tuple": ["deepmind_tracr", "tracr", "utils", "errors.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "ignoring_arithmetic_errors"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def fun_wrapped(*args):\n    try:\n      return fun(*args)\n    except ArithmeticError:\n      logging.warning(\n          \"Encountered arithmetic error in function: for value %s. \"\n          \"Assuming this input will never occur.\", str(args))\n      return None\n", "fpath_tuple": ["deepmind_tracr", "tracr", "utils", "errors.py"], "context_start_lineno": 0, "lineno": 26, "function_name": "fun_wrapped"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 33, "function_name": "_check_layer_naming"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 50, "function_name": "_zero_mlps"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 97, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward_zero(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 135, "function_name": "forward_zero"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 149, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False))\n      return transformer(\n          emb,\n          mask,\n          embedding_size=embedding_size,\n          unembed_at_every_layer=unembed_at_every_layer,\n      )\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 49, "lineno": 227, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward_superposition(emb, mask):\n      return compressed_model.CompressedTransformer(config)(\n          emb,\n          mask,\n          embedding_size=model_size,\n          unembed_at_every_layer=unembed_at_every_layer).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 106, "lineno": 287, "function_name": "forward_superposition"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "default settings.\"\"\"\n      if self.config.layer_norm:\n        return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)\n      return x\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model.py"], "context_start_lineno": 0, "lineno": 83, "function_name": "layer_norm"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def embed(self, tokens: jax.Array) -> jax.Array:\n    token_embeddings = self.token_embed(tokens)\n    positional_embeddings = self.position_embed(jnp.indices(tokens.shape)[-1])\n    return token_embeddings + positional_embeddings  # [B, T, D]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model.py"], "context_start_lineno": 0, "lineno": 171, "function_name": "embed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __call__(\n      self,\n      tokens: jax.Array,\n      use_dropout: bool = True,\n  ) -> CompiledTransformerModelOutput:\n    \"\"\"Embed tokens, pass through model, and unembed output.\"\"\"\n    if self.pad_token is None:\n      input_mask = jnp.ones_like(tokens)\n    else:\n      input_mask = (tokens != self.pad_token)\n    input_embeddings = self.embed(tokens)\n\n    transformer_output = self.transformer(\n        input_embeddings,\n        input_mask,\n        use_dropout=use_dropout,\n    )\n    return CompiledTransformerModelOutput(\n        transformer_output=transformer_output,\n        unembedded_output=self.unembed(\n            transformer_output.output,\n            use_unembed_argmax=self.use_unembed_argmax,\n        ),\n    )\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model.py"], "context_start_lineno": 0, "lineno": 181, "function_name": "__call__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "default settings.\"\"\"\n      if self.config.layer_norm:\n        return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)\n      return x\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model.py"], "context_start_lineno": 0, "lineno": 69, "function_name": "layer_norm"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def __init__(\n      self,\n      basis: Sequence[bases.BasisDirection],\n      enforce_bos: bool = False,\n      bos_token: Optional[str] = None,\n      pad_token: Optional[str] = None,\n      max_seq_len: Optional[int] = None,\n  ):\n    \"\"\"Initialises. If enforce_bos is set, ensures inputs start with it.\"\"\"\n    if enforce_bos and not bos_token:\n      raise ValueError(\"BOS token must be specified if enforcing BOS.\")\n\n    self.encoding_map = {}\n    for i, direction in enumerate(basis):\n      val = direction.value\n      self.encoding_map[val] = i\n\n    if bos_token and bos_token not in self.encoding_map:\n      raise ValueError(\"BOS token missing in encoding.\")\n\n    if pad_token and pad_token not in self.encoding_map:\n      raise ValueError(\"PAD token missing in encoding.\")\n\n    self.enforce_bos = enforce_bos\n    self._bos_token = bos_token\n    self._pad_token = pad_token\n    self._max_seq_len = max_seq_len\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "encoder.py"], "context_start_lineno": 0, "lineno": 76, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def encode(self, inputs: List[bases.Value]) -> List[int]:\n    if self.enforce_bos and inputs[0] != self.bos_token:\n      raise ValueError(\"First input token must be BOS token. \"\n                       f\"Should be '{self.bos_token}', but was '{inputs[0]}'.\")\n    if missing := set(inputs) - set(self.encoding_map.keys()):\n      raise ValueError(f\"Inputs {missing} not found in encoding \",\n                       self.encoding_map.keys())\n    if self._max_seq_len is not None and len(inputs) > self._max_seq_len:\n      raise ValueError(f\"inputs={inputs} are longer than the maximum \"\n                       f\"sequence length {self._max_seq_len}\")\n\n    return [self.encoding_map[x] for x in inputs]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "encoder.py"], "context_start_lineno": 0, "lineno": 96, "function_name": "encode"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def decode(self, encodings: List[int]) -> List[bases.Value]:\n    \"\"\"Recover the tokens that corresponds to `ids`. Inverse of __call__.\"\"\"\n    decoding_map = {val: key for key, val in self.encoding_map.items()}\n    if missing := set(encodings) - set(decoding_map.keys()):\n      raise ValueError(f\"Inputs {missing} not found in decoding map \",\n                       decoding_map.keys())\n    return [decoding_map[x] for x in encodings]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "encoder.py"], "context_start_lineno": 0, "lineno": 110, "function_name": "decode"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "_check_layer_naming"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 46, "function_name": "_zero_mlps"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 61, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 93, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward_zero(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 131, "function_name": "forward_zero"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 145, "function_name": "forward"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _get_one_hot_embed_unembed(self, vocab_size, max_seq_len):\n    # Embeds tokens as one-hot into the first `vocab_size` dimensions\n    token_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.eye(vocab_size),\n             jnp.zeros((vocab_size, max_seq_len))]))\n\n    # Embeds positions as one-hot into the last `max_seq_len` dimensions\n    position_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.zeros((max_seq_len, vocab_size)),\n             jnp.eye(max_seq_len)]))\n\n    class Unembed(hk.Module):\n\n      def __call__(self, embeddings):\n        return jnp.argmax(embeddings[:, :, :vocab_size], axis=-1)\n\n    return token_embed, position_embed, Unembed()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 178, "function_name": "_get_one_hot_embed_unembed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def embed(tokens):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      token_embed, position_embed, unembed = self._get_one_hot_embed_unembed(\n          vocab_size, max_seq_len)\n      compiled_model = model.CompiledTransformerModel(\n          transformer=transformer,\n          token_embed=token_embed,\n          position_embed=position_embed,\n          unembed=unembed,\n          use_unembed_argmax=True,\n          pad_token=pad_token)\n      return compiled_model.embed(tokens)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 32, "lineno": 206, "function_name": "embed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def embed_unembed(tokens):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      token_embed, position_embed, unembed = self._get_one_hot_embed_unembed(\n          vocab_size, max_seq_len)\n      compiled_model = model.CompiledTransformerModel(\n          transformer=transformer,\n          token_embed=token_embed,\n          position_embed=position_embed,\n          unembed=unembed,\n          use_unembed_argmax=True,\n          pad_token=pad_token)\n      embeddings = compiled_model.embed(tokens)\n      unembeddings = compiled_model.unembed(embeddings)\n      return embeddings, unembeddings\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 65, "lineno": 239, "function_name": "embed_unembed"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "defaults\n        to the key size multiplied by the number of heads (K * H).\n      name: Optional name for this module.\n    \"\"\"\n    super().__init__(name=name)\n    self.num_heads = num_heads\n    self.key_size = key_size\n    self.value_size = value_size or key_size\n    self.model_size = model_size or key_size * num_heads\n\n    # Backwards-compatibility for w_init_scale.\n    if w_init_scale is not None:\n      warnings.warn(\n          \"w_init_scale is deprecated; please pass an explicit weight \"\n          \"initialiser instead.\", DeprecationWarning)\n    if w_init and w_init_scale:\n      raise ValueError(\"Please provide only `w_init`, not `w_init_scale`.\")\n    if w_init is None and w_init_scale is None:\n      raise ValueError(\"Please provide a weight initializer: `w_init`.\")\n    if w_init is None:\n      w_init = hk.initializers.VarianceScaling(w_init_scale)\n    self.w_init = w_init\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "attention.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "__init__"}}
{"metadata": {"task_id": "deepmind--tracr/idx", "ground_truth": "def _linear_projection(\n      self,\n      x: jnp.ndarray,\n      head_size: int,\n      name: Optional[str] = None,\n  ) -> jnp.ndarray:\n    y = hk.Linear(self.num_heads * head_size, w_init=self.w_init, name=name)(x)\n    *leading_dims, _ = x.shape\n    return y.reshape((*leading_dims, self.num_heads, head_size))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "attention.py"], "context_start_lineno": 0, "lineno": 157, "function_name": "_linear_projection"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def patch_everything(self):\n        \"\"\"\n        We patch module, optimizer, data loader, and lr scheduler for device placement,\n        distributed training, zero optimizer, fsdp, etc.\n        \"\"\"\n        self.patch_module()\n        self.patch_optimizer()\n        if self.scheduler is not None:\n            self.patch_scheduler()\n        if self.train_data_loader is not None:\n            self.train_data_loader = [\n                self.patch_data_loader(data_loader)\n                for data_loader in self.train_data_loader\n            ]\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 0, "lineno": 201, "function_name": "patch_everything"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def patch_optimizer(self):\n        \"\"\"\n        Patch optimizer given the systems configuration (e.g., DDP, FSDP)\n        \"\"\"\n        params = self.trainable_parameters()\n        if self.is_implemented(\"param_groups\") and self._strategy != \"fsdp\":\n            params = self.param_groups()\n        is_zero = True if self._strategy == \"zero\" else False\n        if self._strategy == \"accelerate\":\n            self.optimizer = self.accelerator.prepare(self.optimizer)\n        else:\n            self.optimizer = patch_optimizer(self.optimizer, params, is_zero)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 23, "lineno": 236, "function_name": "patch_optimizer"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def patch_data_loader(self, loader):\n        \"\"\"\n        Patch data loader given the systems configuration (e.g., DDP, FSDP)\n        \"\"\"\n        if self._strategy in [\"distributed\", \"zero\", \"fsdp\"]:\n            patched_loader = get_distributed_data_loader(\n                loader, world_size=self._world_size, rank=self._rank\n            )\n        elif self._strategy == \"accelerate\":\n            patched_loader = self.accelerator.prepare(loader)\n        else:\n            patched_loader = loader\n\n        return patched_loader\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 61, "lineno": 257, "function_name": "patch_data_loader"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def training_step_exec(self, batch):\n        if self._is_default_fp16():\n            with torch.cuda.amp.autocast():\n                return self.training_step(batch)\n        else:\n            return self.training_step(batch)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 118, "lineno": 312, "function_name": "training_step_exec"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def step(self, global_step=None):\n        \"\"\"\n        ``step`` method abstracts a one-step gradient descent update with four sub-steps:\n        1) data loading, 2) cost calculation, 3) gradient calculation, and 4) parameter update.\n        It also calls upper-level problems' step methods after unrolling gradient steps based on\n        the hierarchical dependency graph.\n\n        :param global_step: global step of the whole multilevel optimization. Defaults to None.\n        :type global_step: int, optional\n        \"\"\"\n        self._global_step = global_step\n        self.step_normal(global_step=global_step)\n        if (\n            self._count % (self._unroll_steps * self.gas) == 0\n            and self._count > self.warmup_steps\n        ):\n            self.step_after_roll_back()\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 217, "lineno": 432, "function_name": "step"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def get_batch(self):\n        \"\"\"\n        Load training batch from the user-provided data loader\n\n        :return: New training batch\n        :rtype: Any\n        \"\"\"\n        batch = tuple(\n            self.get_batch_single_loader(i) for i in range(len(self.train_data_loader))\n        )\n\n        return batch[0] if len(batch) == 1 else batch\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 227, "lineno": 447, "function_name": "get_batch"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def get_batch_single_loader(self, idx):\n        \"\"\"\n        Load training batch from one of the user-provided data loader(s)\n\n        :return: New training batch\n        :rtype: Any\n        \"\"\"\n        data_iterator = self.train_data_iterator[idx]\n        try:\n            batch = next(data_iterator)\n        except StopIteration:\n            if idx == 0:\n                self.epoch_callback_exec()\n            self.epoch_counter[idx] += 1\n            train_data_loader = self.train_data_loader[idx]\n            if self._strategy in [\"distributed\", \"zero\", \"fsdp\"]:\n                train_data_loader.set_epoch(self.epoch_counter[idx])\n            self.train_data_iterator[idx] = iter(train_data_loader)\n            batch = next(self.train_data_iterator[idx])\n        if not isinstance(batch, dict):\n            batch = tuple(\n                convert_tensor(value, self.device, self._is_default_fp16())\n                for value in batch\n            )\n        else:\n            for key, value in batch.items():\n                batch[key] = convert_tensor(value, self.device, self._is_default_fp16())\n\n        return batch\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 236, "lineno": 460, "function_name": "get_batch_single_loader"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "defined loss\n        function.\n\n        :return: loss and log metrics (e.g. classification accuracy)\n        :rtype: dict\n        \"\"\"\n        maybe_loss_dict = self.training_step_exec(batch)\n        is_dict = isinstance(maybe_loss_dict, dict)\n        loss = maybe_loss_dict[\"loss\"] if is_dict else maybe_loss_dict\n        loss_no_scale = loss.item()\n        if self._is_default_fp16():\n            loss = self.scaler.scale(loss)\n        loss = loss / self.gas\n\n        # construct loss dict\n        loss_dict = {\"loss\": loss_no_scale}\n        if is_dict:\n            for key, value in maybe_loss_dict.items():\n                if key != \"loss\":\n                    loss_dict[key] = value\n\n        return loss, loss_dict\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 262, "lineno": 491, "function_name": "get_loss"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def set_grads(self, params, grads):\n        \"\"\"\n        Set gradients for trainable parameters. ``params.grad = grads``\n\n        :param params: Trainable parameters\n        :type params: Sequence of Tensor\n        :param grads: Calculated gradient\n        :type grads: Sequence of Tensor\n        \"\"\"\n        for param, grad in zip(params, grads):\n            if grad is not None:\n                if hasattr(param, \"grad\") and param.grad is not None:\n                    param.grad = param.grad + grad\n                else:\n                    param.grad = grad\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 372, "lineno": 579, "function_name": "set_grads"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def zero_grad(self):\n        \"\"\"\n        Set gradients for trainable parameters for the current problem to 0.\n        Similar with PyTorch's ``optim.zero_grad()`` or ``module.zero_grad()``.\n        \"\"\"\n        for param in list(self.trainable_parameters()):\n            if hasattr(param, \"grad\"):\n                del param.grad\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 401, "lineno": 606, "function_name": "zero_grad"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def configure_distributed_training(self, dictionary):\n        \"\"\"\n        Set the configuration for distributed training.\n\n        :param dictionary: Python dictionary of distributed training provided by Engine.\n        :type dictionary: dict\n        \"\"\"\n        self._strategy = dictionary[\"strategy\"]\n        self._backend = dictionary[\"backend\"]\n        self._world_size = dictionary[\"world_size\"]\n        self._rank = dictionary[\"rank\"]\n        self._local_rank = dictionary[\"local_rank\"]\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 465, "lineno": 658, "function_name": "configure_distributed_training"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "default_fp16(self):\n        \"\"\"\n        Check whether to use PyTorch native fp16 (mixed-precision) feature\n        \"\"\"\n        if not self._fp16 or self._strategy in [\"accelerate\"]:\n            return False\n        return True\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 530, "lineno": 737, "function_name": "_is_default_fp16"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def log(self, stats, global_step):\n        \"\"\"\n        Log (training) stats to the ``self.logger``\n\n        :param stats: log metrics such as loss and classification accuracy.\n        :type stats: Any\n        :param step: global/local step associated with the ``stats``.\n        :type step: int\n        \"\"\"\n        loss_log = log_from_loss_dict(stats)\n        if global_step is None:\n            self.logger.info(\n                f'[Problem \"{self._name}\"] [Local Step {self._count}] {loss_log}'\n            )\n        else:\n            self.logger.info(\n                f'[Problem \"{self._name}\"] [Global Step {global_step}] [Local Step {self._count}] '\n                f\"{loss_log}\"\n            )\n        cur_step = global_step\n        if global_step is None or self.log_local_step:\n            cur_step = self._count\n        self.logger.log(stats, tag=self._name, step=cur_step)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "problem.py"], "context_start_lineno": 550, "lineno": 768, "function_name": "log"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def __init__(\n        self,\n        name,\n        config,\n        module=None,\n        optimizer=None,\n        scheduler=None,\n        train_data_loader=None,\n        extra_config=None,\n    ):\n        super().__init__(\n            name,\n            config,\n            module,\n            optimizer,\n            scheduler,\n            train_data_loader,\n            extra_config,\n        )\n        self.module_state_dict_cache = None\n        self.opitmizer_state_dict_cache = None\n", "fpath_tuple": ["leopard-ai_betty", "betty", "problems", "implicit_problem.py"], "context_start_lineno": 0, "lineno": 24, "function_name": "__init__"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def convert_tensor(item, device=None, fp16=False):\n    if not isinstance(item, torch.Tensor):\n        return item\n    return item.to(device)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "utils.py"], "context_start_lineno": 0, "lineno": 4, "function_name": "convert_tensor"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def log_from_loss_dict(loss_dict):\n    outputs = []\n    for key, values in loss_dict.items():\n        if isinstance(values, dict) or isinstance(values, list):\n            for value_idx, value in enumerate(values):\n                full_key = key + \"_\" + str(value_idx)\n                if torch.is_tensor(value):\n                    value = value.item()\n                output = f\"{full_key}: {value}\"\n                outputs.append(output)\n        else:\n            if torch.is_tensor(values):\n                values = values.item()\n            output = f\"{key}: {values}\"\n            outputs.append(output)\n    return \" || \".join(outputs)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "utils.py"], "context_start_lineno": 0, "lineno": 90, "function_name": "log_from_loss_dict"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def neg_with_none(a):\n    if a is None:\n        return None\n    else:\n        return -a\n", "fpath_tuple": ["leopard-ai_betty", "betty", "utils.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "neg_with_none"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def replace_none_with_zero(tensor_list, reference):\n    out = []\n    for t, r in zip(tensor_list, reference):\n        fixed = t if t is not None else torch.zeros_like(r)\n        out.append(fixed)\n    return tuple(out)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "utils.py"], "context_start_lineno": 0, "lineno": 123, "function_name": "replace_none_with_zero"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def parse_config(self):\n        \"\"\"\n        Parse EngineConfig.\n        \"\"\"\n        self.train_iters = self.config.train_iters\n        self.valid_step = self.config.valid_step\n\n        self.logger_type = self.config.logger_type\n\n        self._roll_back = self.config.roll_back\n\n        self._strategy = self.config.strategy\n        self._backend = self.config.backend\n\n        if self.config.early_stopping:\n            self.early_stopping = EarlyStopping(\n                metric=self.config.early_stopping_metric,\n                mode=self.config.early_stopping_mode,\n                tolerance=self.config.early_stopping_tolerance,\n            )\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "parse_config"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def check_leaf(self, problem):\n        \"\"\"\n        Check whether the given ``problem`` is a leaf problem or not.\n\n        :param problem: Problem in multilevel optimization\n        :type problem: Problem\n        :return: True or False\n        :rtype: bool\n        \"\"\"\n        for _, value_list in self.dependencies[\"l2u\"].items():\n            if problem in set(value_list):\n                return False\n\n        return True\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 0, "lineno": 223, "function_name": "check_leaf"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def find_paths(self, src, dst):\n        \"\"\"\n        Find all paths from ``src`` to ``dst`` with a modified depth-first search algorithm.\n\n        :param src: The end point of the upper-to-lower edge.\n        :type src: Problem\n        :param dst: The start point of the upper-to-lower edge.\n        :type dst: Problem\n        :return: List of all paths from ``src`` to ``dst``.\n        \"\"\"\n        results = []\n        path = [src]\n        self.dfs(src, dst, path, results)\n        assert len(results) > 0, f\"No path from {src.name} to {dst.name}!\"\n\n        for i, _ in enumerate(results):\n            results[i].reverse()\n            results[i].append(dst)\n\n        return results\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 0, "lineno": 239, "function_name": "find_paths"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def dfs(self, src, dst, path, results):\n        if src is dst:\n            assert len(path) > 1\n            result = [node for node in path]\n            results.append(result)\n        elif src not in self.dependencies[\"l2u\"]:\n            return\n        else:\n            for adj in self.dependencies[\"l2u\"][src]:\n                path.append(adj)\n                self.dfs(adj, dst, path, results)\n                path.pop()\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 11, "lineno": 251, "function_name": "dfs"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def parse_dependency(self):\n        \"\"\"\n        Parse user-provided ``u2l`` and ``l2u`` dependencies to figure out 1) topological order for\n        multilevel optimization execution, and 2) backpropagation path(s) for each problem. A\n        modified depth-first search algorithm is used.\n        \"\"\"\n        # Parse upper-to-lower dependency\n        for key, value_list in self.dependencies[\"u2l\"].items():\n            for value in value_list:\n                # find all paths from low to high for backpropagation\n                paths = self.find_paths(src=value, dst=key)\n                key.add_paths(paths)\n\n        # Parse lower-to-upper dependency\n        for key, value_list in self.dependencies[\"l2u\"].items():\n            for value in value_list:\n                # add value problem to parents of key problem for backpropgation\n                key.add_parent(value)\n                value.add_child(key)\n\n        # Parse problems\n        for problem in self.problems:\n            if self.check_leaf(problem):\n                problem.leaf = True\n                self.leaves.append(problem)\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 37, "lineno": 270, "function_name": "parse_dependency"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def set_problem_attr(self, problem):\n        \"\"\"\n        Set class attribute for the given ``problem`` based on their names\n\n        :param problem: Problem in multilevel optimization\n        :type problem: Problem\n        :return: ``problem`` name\n        :rtype: str\n        \"\"\"\n        name = problem.name\n\n        # set attribute for Engine\n        assert not hasattr(self, name), f\"Problem already has a problelm named {name}!\"\n        setattr(self, name, problem)\n\n        # set attribute for Problems\n        for prob in self.problems:\n            if prob != problem:\n                assert not hasattr(problem, name)\n                setattr(prob, name, problem)\n\n        # set attribute for Env\n        if self.env is not None:\n            setattr(self.env, name, problem)\n\n        return name\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 88, "lineno": 309, "function_name": "set_problem_attr"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def do_validation(self):\n        \"\"\"\n        Check whether to run validation.\n        \"\"\"\n        if self.is_implemented(\"validation\") and self.is_rank_zero():\n            return True\n        return False\n", "fpath_tuple": ["leopard-ai_betty", "betty", "engine.py"], "context_start_lineno": 105, "lineno": 331, "function_name": "do_validation"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def patch_optimizer(optimizer, params, is_zero):\n    defaults = optimizer.defaults\n    new_optimizer = None\n    if is_zero:\n        new_optimizer = ZeroRedundancyOptimizer(\n            params=params,\n            optimizer_class=optimizer.__class__,\n            parameters_as_bucket_view=True,\n            **defaults,\n        )\n    else:\n        new_optimizer = optimizer.__class__(params, **defaults)\n\n    return new_optimizer\n", "fpath_tuple": ["leopard-ai_betty", "betty", "patch", "optimizer.py"], "context_start_lineno": 0, "lineno": 4, "function_name": "patch_optimizer"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def neumann(vector, curr, prev, sync):\n    \"\"\"\n    Approximate the matrix-vector multiplication with the best response Jacobian by the\n    Neumann Series as proposed in\n    `Optimizing Millions of Hyperparameters by Implicit Differentiation\n    <https://arxiv.org/abs/1911.02590>`_ based on implicit function theorem (IFT). Users may\n    specify learning rate (``neumann_alpha``) and unrolling steps (``neumann_iterations``) in\n    ``Config``.\n\n    :param vector:\n        Vector with which matrix-vector multiplication with best-response Jacobian (matrix) would\n        be performed.\n    :type vector: Sequence of Tensor\n    :param curr: A current level problem\n    :type curr: Problem\n    :param prev: A directly lower-level problem to the current problem\n    :type prev: Problem\n    :return: (Intermediate) gradient\n    :rtype: Sequence of Tensor\n    \"\"\"\n    # ! Mabye replace with child.loss by adding self.loss attribute to save computation\n    assert len(curr.paths) == 0, \"neumann method is not supported for higher order MLO!\"\n    config = curr.config\n    in_loss = curr.training_step_exec(curr.cur_batch)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        in_grad = torch.autograd.grad(\n            in_loss, curr.trainable_parameters(), create_graph=True\n        )\n    v2 = approx_inverse_hvp(\n        vector,\n        in_grad,\n        curr.trainable_parameters(),\n        iterations=config.neumann_iterations,\n        alpha=config.neumann_alpha,\n    )\n    if sync:\n        v2 = [neg_with_none(x) for x in v2]\n        torch.autograd.backward(\n            in_grad, inputs=prev.trainable_parameters(), grad_tensors=v2\n        )\n        implicit_grad = None\n    else:\n        implicit_grad = torch.autograd.grad(\n            in_grad, prev.trainable_parameters(), grad_outputs=v2\n        )\n        implicit_grad = [neg_with_none(ig) for ig in implicit_grad]\n\n    return implicit_grad\n", "fpath_tuple": ["leopard-ai_betty", "betty", "hypergradient", "neumann.py"], "context_start_lineno": 0, "lineno": 28, "function_name": "neumann"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def approx_inverse_hvp(v, f, params, iterations=3, alpha=1.0):\n    p = v\n    for _ in range(iterations):\n        hvp = torch.autograd.grad(f, params, grad_outputs=v, retain_graph=True)\n        v = [v_i - alpha * hvp_i for v_i, hvp_i in zip(v, hvp)]\n        p = [v_i + p_i for v_i, p_i in zip(v, p)]\n\n    return [alpha * p_i for p_i in p]\n", "fpath_tuple": ["leopard-ai_betty", "betty", "hypergradient", "neumann.py"], "context_start_lineno": 0, "lineno": 59, "function_name": "approx_inverse_hvp"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def get_optimzer_type(optimizer):\n    cls_name = type(optimizer).__name__.lower()\n    if \"adam\" in cls_name:\n        return \"adam\"\n    return \"sgd\"\n", "fpath_tuple": ["leopard-ai_betty", "betty", "hypergradient", "utils.py"], "context_start_lineno": 0, "lineno": 5, "function_name": "get_optimzer_type"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def get_grads(loss, path, retain_graph, do_sync):\n    jvp = torch.autograd.grad(\n        loss,\n        path[1].trainable_parameters(),\n        retain_graph=retain_graph,\n        allow_unused=True,\n    )\n    jvp = replace_none_with_zero(jvp, path[1].trainable_parameters())\n    for i in range(1, len(path) - 1):\n        jvp_fn_type = path[i].config.type\n        assert jvp_fn_type in jvp_fn_mapping\n        jvp_fn = jvp_fn_mapping[jvp_fn_type]\n        sync = bool(do_sync and i == len(path) - 2)\n        jvp = jvp_fn(jvp, path[i], path[i + 1], sync)\n\n    return jvp\n", "fpath_tuple": ["leopard-ai_betty", "betty", "hypergradient", "__init__.py"], "context_start_lineno": 0, "lineno": 19, "function_name": "get_grads"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def make_data_loader(xs, ys):\n    datasets = [(xs, ys)]\n\n    return datasets\n", "fpath_tuple": ["leopard-ai_betty", "betty", "test_install.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "make_data_loader"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def __init__(self) -> None:\n        super().__init__()\n\n        self.w = torch.nn.Parameter(torch.zeros(DATA_DIM))\n", "fpath_tuple": ["leopard-ai_betty", "betty", "test_install.py"], "context_start_lineno": 0, "lineno": 39, "function_name": "__init__"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def __init__(self) -> None:\n        super().__init__()\n\n        self.w = torch.nn.Parameter(torch.ones(DATA_DIM))\n", "fpath_tuple": ["leopard-ai_betty", "betty", "test_install.py"], "context_start_lineno": 0, "lineno": 50, "function_name": "__init__"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def training_step(self, batch):\n        inputs, targets = batch\n        outs = self.inner(inputs)[0]\n        loss = F.binary_cross_entropy_with_logits(outs, targets)\n        return loss\n", "fpath_tuple": ["leopard-ai_betty", "betty", "test_install.py"], "context_start_lineno": 0, "lineno": 60, "function_name": "training_step"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def training_step(self, batch):\n        inputs, targets = batch\n        outs, params = self.module(inputs)\n        loss = (\n            F.binary_cross_entropy_with_logits(outs, targets)\n            + 0.5\n            * (\n                params.unsqueeze(0) @ torch.diag(self.outer()) @ params.unsqueeze(1)\n            ).sum()\n        )\n        return loss\n", "fpath_tuple": ["leopard-ai_betty", "betty", "test_install.py"], "context_start_lineno": 0, "lineno": 81, "function_name": "training_step"}}
{"metadata": {"task_id": "leopard-ai--betty/idx", "ground_truth": "def get_logger():\n    \"\"\"\n    Get global logger.\n    \"\"\"\n    global _logger\n    if _logger:\n        return _logger\n    logger = logging.getLogger(\"betty\")\n    log_format = logging.Formatter(\n        \"[%(asctime)s] [%(levelname)s] %(message)s\", \"%Y-%m-%d %H:%M:%S\"\n    )\n\n    logger.propagate = False\n    logger.setLevel(logging.INFO)\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.INFO)\n    ch.setFormatter(log_format)\n    logger.addHandler(ch)\n\n    _logger = logger\n    return _logger\n", "fpath_tuple": ["leopard-ai_betty", "betty", "logging", "logger_base.py"], "context_start_lineno": 0, "lineno": 12, "function_name": "get_logger"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "_make_single_prediction"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 0, "lineno": 138, "function_name": "_calculate_number_rows_plot"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:\n  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n\n  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 0, "lineno": 161, "function_name": "_calculate_media_contribution"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 408, "lineno": 571, "function_name": "plot_cross_correlate"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 430, "lineno": 597, "function_name": "plot_var_cost"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 520, "lineno": 688, "function_name": "_call_fit_plotter"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 561, "lineno": 728, "function_name": "plot_model_fit"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,\n                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,\n      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 603, "lineno": 763, "function_name": "plot_out_of_sample_model_fit"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "context_start_lineno": 0, "lineno": 96, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(data):\n      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "context_start_lineno": 0, "lineno": 168, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "context_start_lineno": 51, "lineno": 213, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "_sinusoidal_seasonality"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "context_start_lineno": 0, "lineno": 84, "function_name": "sinusoidal_seasonality"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones.\n\n  Returns:\n    The contribution of the weekday seasonality.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(name=f\"{priors.WEEKDAY}_plate\", size=7):\n    weekday = numpyro.sample(\n        name=priors.WEEKDAY,\n        fn=custom_priors.get(priors.WEEKDAY, default_priors[priors.WEEKDAY]))\n\n  weekday_series = _intra_week_seasonality(data=data, weekday=weekday)\n\n  if data.ndim == 3:  # For geo model's case\n    weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n\n  return weekday_series\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "context_start_lineno": 0, "lineno": 130, "function_name": "intra_week_seasonality"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "context_start_lineno": 0, "lineno": 59, "function_name": "trend_with_exponent"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,\n    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:\n    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.\n  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])\n  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "context_start_lineno": 0, "lineno": 107, "function_name": "_dynamic_trend"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones. Refer to the full documentation on custom priors for\n      details.\n\n  Returns:\n    The values of the intercept.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n\n  with numpyro.plate(name=f\"{priors.INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=priors.INTERCEPT,\n        fn=custom_priors.get(priors.INTERCEPT,\n                             default_priors[priors.INTERCEPT]),\n    )\n  return intercept\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "context_start_lineno": 0, "lineno": 45, "function_name": "simple_intercept"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(data):\n      numpyro.deterministic(\n          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "context_start_lineno": 0, "lineno": 44, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "context_start_lineno": 0, "lineno": 45, "function_name": "_hill"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "context_start_lineno": 0, "lineno": 69, "function_name": "hill"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.EXPONENT}_plate\", size=data.shape[1]):\n    exponent_values = numpyro.sample(\n        name=f\"{prefix}{priors.EXPONENT}\",\n        fn=custom_priors.get(priors.EXPONENT, default_priors[priors.EXPONENT]))\n\n  if data.ndim == 3:\n    exponent_values = jnp.expand_dims(exponent_values, axis=-1)\n  return _exponent(data=data, exponent_values=exponent_values)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "context_start_lineno": 0, "lineno": 119, "function_name": "exponent"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "_carryover"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 0, "lineno": 102, "function_name": "carryover"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 0, "lineno": 146, "function_name": "_adstock"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.LAG_WEIGHT}_plate\", size=data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=f\"{prefix}{priors.LAG_WEIGHT}\",\n        fn=custom_priors.get(priors.LAG_WEIGHT,\n                             default_priors[priors.LAG_WEIGHT]))\n\n  if data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n\n  return _adstock(data=data, lag_weight=lag_weight, normalise=normalise)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 6, "lineno": 185, "function_name": "adstock"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(data, normalise):\n      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "context_start_lineno": 0, "lineno": 168, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "_get_default_priors"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "context_start_lineno": 0, "lineno": 113, "function_name": "_get_transform_default_priors"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n\n  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "transform_adstock"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "context_start_lineno": 0, "lineno": 89, "function_name": "_set_up_mock_mmm"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "context_start_lineno": 0, "lineno": 118, "function_name": "setUpClass"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def setUp(self):\n    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))\n    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "context_start_lineno": 0, "lineno": 138, "function_name": "setUp"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "context_start_lineno": 0, "lineno": 66, "function_name": "_objective_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "context_start_lineno": 0, "lineno": 126, "function_name": "_get_lower_and_upper_bounds"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last\n  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n\n  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "context_start_lineno": 0, "lineno": 171, "function_name": "_generate_starting_values"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 0, "lineno": 110, "function_name": "fit"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 0, "lineno": 133, "function_name": "transform"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 60, "lineno": 244, "function_name": "_compute_variances"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 92, "lineno": 282, "function_name": "_compute_spend_fractions"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:\n  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")\n\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)\n  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 117, "lineno": 313, "function_name": "_compute_variance_inflation_factors"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 0, "lineno": 56, "function_name": "load_model"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 0, "lineno": 183, "function_name": "_split_array_into_list"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 129, "lineno": 301, "function_name": "get_beta_params_from_mu_sigma"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 145, "lineno": 333, "function_name": "_pmf"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 164, "lineno": 357, "function_name": "distance_pior_posterior"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def interpolate_outliers(x: jnp.ndarray,\n                         outlier_idx: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Overwrites outliers in x with interpolated values.\n\n  Args:\n    x: The original univariate variable with outliers.\n    outlier_idx: Indices of the outliers in x.\n\n  Returns:\n    A cleaned x with outliers overwritten.\n\n  \"\"\"\n  time_idx = jnp.arange(len(x))\n  inverse_idx = jnp.array([i for i in range(len(x)) if i not in outlier_idx])\n  interp_func = interpolate.interp1d(\n      time_idx[inverse_idx], x[inverse_idx], kind=\"linear\")\n  x = x.at[outlier_idx].set(interp_func(time_idx[outlier_idx]))\n  return x\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 210, "lineno": 391, "function_name": "interpolate_outliers"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "context_start_lineno": 0, "lineno": 39, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "context_start_lineno": 0, "lineno": 64, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "context_start_lineno": 0, "lineno": 89, "function_name": "mock_model_function"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "setUpClass"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def setUp(self):\n    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "context_start_lineno": 0, "lineno": 51, "function_name": "setUp"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "calculate_seasonality"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "adstock"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 112, "function_name": "hill"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 158, "function_name": "carryover"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def setUpClass(cls):\n    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "setUpClass"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "_compare_equality_for_lmmm"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 0, "lineno": 167, "function_name": "__post_init__"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "defined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n\n    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 9, "lineno": 192, "function_name": "__eq__"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 14, "lineno": 197, "function_name": "_create_list_of_attributes_to_compare"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 272, "lineno": 441, "function_name": "_predict"}}
{"metadata": {"task_id": "google--lightweight_mmm/idx", "ground_truth": "def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n\n    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"\n    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]\n    logging.info(\"Reduction is complete\")\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 371, "lineno": 548, "function_name": "reduce_trace"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(self, percentage: float):\n        if not 0 < percentage < 1:\n            raise ValueError(\"Percentage value not in (0, 1).\")\n        self.percentage = percentage\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "context_start_lineno": 0, "lineno": 17, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(\n        self,\n        percentage: float,\n        device: torch.device,\n        dimension_to_project_features_to=128,\n    ):\n        \"\"\"Greedy Coreset sampling base class.\"\"\"\n        super().__init__(percentage)\n\n        self.device = device\n        self.dimension_to_project_features_to = dimension_to_project_features_to\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "sampler.py"], "context_start_lineno": 0, "lineno": 46, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _detach(features):\n            if detach:\n                return [x.detach().cpu().numpy() for x in features]\n            return features\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "_detach"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _fill_memory_bank(self, input_data):\n        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n        _ = self.forward_modules.eval()\n\n        def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n\n        features = []\n        with tqdm.tqdm(\n            input_data, desc=\"Computing support features...\", position=1, leave=False\n        ) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    image = image[\"image\"]\n                features.append(_image_to_features(image))\n\n        features = np.concatenate(features, axis=0)\n        features = self.featuresampler.run(features)\n\n        self.anomaly_scorer.fit(detection_features=[features])\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "_fill_memory_bank"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _image_to_features(input_image):\n            with torch.no_grad():\n                input_image = input_image.to(torch.float).to(self.device)\n                return self._embed(input_image)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 159, "function_name": "_image_to_features"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def predict(self, data):\n        if isinstance(data, torch.utils.data.DataLoader):\n            return self._predict_dataloader(data)\n        return self._predict(data)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 178, "function_name": "predict"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _predict_dataloader(self, dataloader):\n        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n        _ = self.forward_modules.eval()\n\n        scores = []\n        masks = []\n        labels_gt = []\n        masks_gt = []\n        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n            for image in data_iterator:\n                if isinstance(image, dict):\n                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n                    image = image[\"image\"]\n                _scores, _masks = self._predict(image)\n                for score, mask in zip(_scores, _masks):\n                    scores.append(score)\n                    masks.append(mask)\n        return scores, masks, labels_gt, masks_gt\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 0, "lineno": 184, "function_name": "_predict_dataloader"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n        LOGGER.info(\"Saving PatchCore data.\")\n        self.anomaly_scorer.save(\n            save_path, save_features_separately=False, prepend=prepend\n        )\n        patchcore_params = {\n            \"backbone.name\": self.backbone.name,\n            \"layers_to_extract_from\": self.layers_to_extract_from,\n            \"input_shape\": self.input_shape,\n            \"pretrain_embed_dimension\": self.forward_modules[\n                \"preprocessing\"\n            ].output_dim,\n            \"target_embed_dimension\": self.forward_modules[\n                \"preadapt_aggregator\"\n            ].target_dim,\n            \"patchsize\": self.patch_maker.patchsize,\n            \"patchstride\": self.patch_maker.stride,\n            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n        }\n        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 57, "lineno": 234, "function_name": "save_to_path"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def patchify(self, features, return_spatial_info=False):\n        \"\"\"Convert a tensor into a tensor of respective patches.\n        Args:\n            x: [torch.Tensor, bs x c x w x h]\n        Returns:\n            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n            patchsize]\n        \"\"\"\n        padding = int((self.patchsize - 1) / 2)\n        unfolder = torch.nn.Unfold(\n            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n        )\n        unfolded_features = unfolder(features)\n        number_of_total_patches = []\n        for s in features.shape[-2:]:\n            n_patches = (\n                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n            ) / self.stride + 1\n            number_of_total_patches.append(int(n_patches))\n        unfolded_features = unfolded_features.reshape(\n            *features.shape[:2], self.patchsize, self.patchsize, -1\n        )\n        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n\n        if return_spatial_info:\n            return unfolded_features, number_of_total_patches\n        return unfolded_features\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 120, "lineno": 289, "function_name": "patchify"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def score(self, x):\n        was_numpy = False\n        if isinstance(x, np.ndarray):\n            was_numpy = True\n            x = torch.from_numpy(x)\n        while x.ndim > 1:\n            x = torch.max(x, dim=-1).values\n        if was_numpy:\n            return x.numpy()\n        return x\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "patchcore.py"], "context_start_lineno": 140, "lineno": 313, "function_name": "score"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n        \"\"\"FAISS Nearest neighbourhood search.\n\n        Args:\n            on_gpu: If set true, nearest neighbour searches are done on GPU.\n            num_workers: Number of workers to use with FAISS for similarity search.\n        \"\"\"\n        faiss.omp_set_num_threads(num_workers)\n        self.on_gpu = on_gpu\n        self.search_index = None\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 21, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _index_to_gpu(self, index):\n        if self.on_gpu:\n            # For the non-gpu faiss python package, there is no GpuClonerOptions\n            # so we can not make a default in the function header.\n            return faiss.index_cpu_to_gpu(\n                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n            )\n        return index\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 29, "function_name": "_index_to_gpu"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _index_to_cpu(self, index):\n        if self.on_gpu:\n            return faiss.index_gpu_to_cpu(index)\n        return index\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 38, "function_name": "_index_to_cpu"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _create_index(self, dimension):\n        if self.on_gpu:\n            return faiss.GpuIndexFlatL2(\n                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n            )\n        return faiss.IndexFlatL2(dimension)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 43, "function_name": "_create_index"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def fit(self, features: np.ndarray) -> None:\n        \"\"\"\n        Adds features to the FAISS search index.\n\n        Args:\n            features: Array of size NxD.\n        \"\"\"\n        if self.search_index:\n            self.reset_index()\n        self.search_index = self._create_index(features.shape[-1])\n        self._train(self.search_index, features)\n        self.search_index.add(features)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 56, "function_name": "fit"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def run(\n        self,\n        n_nearest_neighbours,\n        query_features: np.ndarray,\n        index_features: np.ndarray = None,\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns distances and indices of nearest neighbour search.\n\n        Args:\n            query_features: Features to retrieve.\n            index_features: [optional] Index features to search in.\n        \"\"\"\n        if index_features is None:\n            return self.search_index.search(query_features, n_nearest_neighbours)\n\n        # Build a search index just for this search.\n        search_index = self._create_index(index_features.shape[-1])\n        self._train(search_index, index_features)\n        search_index.add(index_features)\n        return search_index.search(query_features, n_nearest_neighbours)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 78, "function_name": "run"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def reset_index(self):\n        if self.search_index:\n            self.search_index.reset()\n            self.search_index = None\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "reset_index"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _create_index(self, dimension):\n        index = faiss.IndexIVFPQ(\n            faiss.IndexFlatL2(dimension),\n            dimension,\n            512,  # n_centroids\n            64,  # sub-quantizers\n            8,\n        )  # nbits per code\n        return self._index_to_gpu(index)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 109, "function_name": "_create_index"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def _reduce(features):\n        # NxCxWxH -> NxC\n        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n            axis=-1\n        )\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 132, "function_name": "_reduce"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(self, input_dims, output_dim):\n        super(Preprocessing, self).__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n\n        self.preprocessing_modules = torch.nn.ModuleList()\n        for input_dim in input_dims:\n            module = MeanMapper(output_dim)\n            self.preprocessing_modules.append(module)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 146, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def forward(self, features):\n        _features = []\n        for module, feature in zip(self.preprocessing_modules, features):\n            _features.append(module(feature))\n        return torch.stack(_features, dim=1)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "forward"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def forward(self, features):\n        \"\"\"Returns reshaped and average pooled features.\"\"\"\n        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n        features = features.reshape(len(features), 1, -1)\n        features = F.adaptive_avg_pool1d(features, self.target_dim)\n        return features.reshape(len(features), -1)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 180, "function_name": "forward"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(self, device, target_size=224):\n        self.device = device\n        self.target_size = target_size\n        self.smoothing = 4\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 187, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def convert_to_segmentation(self, patch_scores):\n        with torch.no_grad():\n            if isinstance(patch_scores, np.ndarray):\n                patch_scores = torch.from_numpy(patch_scores)\n            _scores = patch_scores.to(self.device)\n            _scores = _scores.unsqueeze(1)\n            _scores = F.interpolate(\n                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n            )\n            _scores = _scores.squeeze(1)\n            patch_scores = _scores.cpu().numpy()\n\n        return [\n            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n            for patch_score in patch_scores\n        ]\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 0, "lineno": 193, "function_name": "convert_to_segmentation"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def forward(self, images):\n        self.outputs.clear()\n        with torch.no_grad():\n            # The backbone will throw an Exception once it reached the last\n            # layer to compute features from. Computation will stop there.\n            try:\n                _ = self.backbone(images)\n            except LastLayerToExtractReachedException:\n                pass\n        return self.outputs\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 58, "lineno": 259, "function_name": "forward"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def feature_dimensions(self, input_shape):\n        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n        _input = torch.ones([1] + list(input_shape)).to(self.device)\n        _output = self(_input)\n        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 70, "lineno": 271, "function_name": "feature_dimensions"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n        self.hook_dict = hook_dict\n        self.layer_name = layer_name\n        self.raise_exception_to_break = copy.deepcopy(\n            layer_name == last_layer_to_extract\n        )\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 82, "lineno": 278, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __call__(self, module, input, output):\n        self.hook_dict[self.layer_name] = output\n        if self.raise_exception_to_break:\n            raise LastLayerToExtractReachedException()\n        return None\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 87, "lineno": 285, "function_name": "__call__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n        \"\"\"\n        Neearest-Neighbourhood Anomaly Scorer class.\n\n        Args:\n            n_nearest_neighbours: [int] Number of nearest neighbours used to\n                determine anomalous pixels.\n            nn_method: Nearest neighbour search method.\n        \"\"\"\n        self.feature_merger = ConcatMerger()\n\n        self.n_nearest_neighbours = n_nearest_neighbours\n        self.nn_method = nn_method\n\n        self.imagelevel_nn = lambda query: self.nn_method.run(\n            n_nearest_neighbours, query\n        )\n        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 105, "lineno": 305, "function_name": "__init__"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def fit(self, detection_features: List[np.ndarray]) -> None:\n        \"\"\"Calls the fit function of the nearest neighbour method.\n\n        Args:\n            detection_features: [list of np.arrays]\n                [[bs x d_i] for i in n] Contains a list of\n                np.arrays for all training images corresponding to respective\n                features VECTORS (or maps, but will be resized) produced by\n                some backbone network which should be used for image-level\n                anomaly detection.\n        \"\"\"\n        self.detection_features = self.feature_merger.merge(\n            detection_features,\n        )\n        self.nn_method.fit(self.detection_features)\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 134, "lineno": 326, "function_name": "fit"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def predict(\n        self, query_features: List[np.ndarray]\n    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Predicts anomaly score.\n\n        Searches for nearest neighbours of test images in all\n        support training images.\n\n        Args:\n             detection_query_features: [dict of np.arrays] List of np.arrays\n                 corresponding to the test features generated by\n                 some backbone network.\n        \"\"\"\n        query_features = self.feature_merger.merge(\n            query_features,\n        )\n        query_distances, query_nns = self.imagelevel_nn(query_features)\n        anomaly_scores = np.mean(query_distances, axis=-1)\n        return anomaly_scores, query_distances, query_nns\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 152, "lineno": 344, "function_name": "predict"}}
{"metadata": {"task_id": "amazon-science--patchcore-inspection/idx", "ground_truth": "def save(\n        self,\n        save_folder: str,\n        save_features_separately: bool = False,\n        prepend: str = \"\",\n    ) -> None:\n        self.nn_method.save(self._index_file(save_folder, prepend))\n        if save_features_separately:\n            self._save(\n                self._detection_file(save_folder, prepend), self.detection_features\n            )\n", "fpath_tuple": ["amazon-science_patchcore-inspection", "src", "patchcore", "common.py"], "context_start_lineno": 181, "lineno": 377, "function_name": "save"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 23, "function_name": "_unix_pattern_to_parameter_names"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 52, "function_name": "__call__"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 81, "function_name": "__call__"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 168, "function_name": "__init__"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 179, "function_name": "__call__"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 53, "lineno": 266, "function_name": "load_checkpoint_and_apply_kernels"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:\n            raise KeyError(err)\n    return model\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 100, "lineno": 305, "function_name": "load_state_dict_into_model"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 0, "lineno": 170, "function_name": "hook_fn"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 0, "lineno": 187, "function_name": "_get_trunk_fields"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 0, "lineno": 201, "function_name": "forward_sub_batch"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def forward(self, batch, *args, **kwargs) -> Dict:\n        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):\n                # FIXME: this only handles list inputs for the field \"vision\"\n                assert len(batch) == 1\n                out_vals = []\n                for e in sub_batch.vision:\n                    e_batch = copy.copy(sub_batch)\n                    e_batch.vision = e\n                    self.forward_sub_batch(e_batch, *args, **kwargs)\n                    assert len(self.outputs) == 1\n                    out_key, out_val = self.outputs.popitem()\n                    out_vals.append(out_val)\n                return {out_key: torch.cat(out_vals)}\n            else:\n                self.forward_sub_batch(sub_batch, *args, **kwargs)\n        outputs = self.outputs\n        self.input_key = None\n        self.outputs = {}\n        return outputs\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 18, "lineno": 210, "function_name": "forward"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"\n    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "create_batch_sample_cls"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def __post_init__(self):\n        # amp\n        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "__post_init__"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 28, "function_name": "validate_param_group_params"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 40, "function_name": "unix_pattern_to_parameter_names"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "get_full_parameter_name"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "unix_param_pattern_to_parameter_names"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 104, "function_name": "set_default_parameters"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:\n    schedulers = []\n    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]\n            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 134, "function_name": "map_scheduler_cfgs_to_param_groups"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "__init__"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def _validate_optimizer_schedulers(self):\n        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "context_start_lineno": 0, "lineno": 15, "function_name": "_validate_optimizer_schedulers"}}
{"metadata": {"task_id": "facebookresearch--omnivore/idx", "ground_truth": "def step_schedulers(self, where: float) -> None:\n        if self.schedulers is None:\n            return\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            for option, scheduler in self.schedulers[i].items():\n                new_value = scheduler(where)\n                param_group[option] = new_value\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "context_start_lineno": 0, "lineno": 26, "function_name": "step_schedulers"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def mutate(df: PandasDataFrame, over: dict[Column, Func]) -> PandasDataFrame:\n    _check_type(over, dict)\n    df = df.copy()\n    for column, mutation in over.items():\n        df[column] = df.apply(mutation, axis=1)\n    return df  # type: ignore\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "mutate.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "mutate"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def drop(df: PandasDataFrame, columns: LazyColumns) -> PandasDataFrame:\n    _check_type(columns, {list, str})\n    df = df.drop(columns, axis=1)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "drop.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "drop"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def rename(df: PandasDataFrame, columns: dict[OldColumn, NewColumn]) -> PandasDataFrame:\n    _check_type(columns, dict)\n    cv = columns.values()\n    _check_values(cv, str)\n    if len(set(cv)) != len(cv):\n        raise KeyError(\"columns must be unique\")\n    missing_keys = set(columns.keys()) - set(df.columns)\n    if missing_keys and len(missing_keys) == 1:\n        raise KeyError(f\"column key ({missing_keys}) is invalid\")\n    if missing_keys and len(missing_keys) > 1:\n        raise KeyError(f\"column keys ({missing_keys}) are invalid\")\n    df = df.rename(columns=columns)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "rename.py"], "context_start_lineno": 0, "lineno": 5, "function_name": "rename"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def split(\n    df: PandasDataFrame, column: Column, into: Columns, sep: str, drop: bool = True\n) -> PandasDataFrame:\n    _check_type(column, str)\n    _check_type(into, list)\n    _check_type(sep, str)\n    _check_type(drop, bool)\n    if len(into) != len(set(into)):\n        raise KeyError(\"into keys must be unique\")\n    if (column in into) and (not drop):\n        raise KeyError(\"into keys must be unique\")\n    bad_keys = set(df.columns).difference(set([column])).intersection(set(into))\n    if bad_keys:\n        raise KeyError(\"into keys must be unique\")\n    columns = {uuid.uuid4().hex: col for col in into}\n    temp = list(columns.keys())\n    df = df.copy()\n    df[temp] = df[column].str.split(sep, expand=True)\n    if drop:\n        df = df.drop(column, axis=1)\n    df = df.rename(columns=columns)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "split.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "split"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def sort(\n    df: PandasDataFrame, columns: LazyColumns, descending: bool = False\n) -> PandasDataFrame:\n    _check_type(columns, {list, str})\n    _check_type(descending, bool)\n    _check_keys(columns, df.columns)\n    df = df.sort_values(by=columns, ascending=not descending)\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "sort.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "sort"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def pack(\n    df: PandasDataFrame | PandasGroupedFrame, column: Column, sep: str\n) -> PandasDataFrame:\n    _check_type(column, str)\n    _check_type(sep, str)\n    order = df.obj.columns if isinstance(df, PandasGroupedFrame) else df.columns  # type: ignore\n    df = df.agg(**{column: (column, lambda x: x.astype(str).str.cat(sep=sep))})  # type: ignore\n    df = df[[col for col in df.columns if col in order]]\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "pack.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "pack"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def rollup(\n    df: PandasDataFrame | PandasGroupedFrame,\n    over: dict[Column, tuple[Column, Func]],\n) -> PandasDataFrame:\n    _check_type(over, dict)\n    if isinstance(df, PandasGroupedFrame):\n        groups = set(df.grouper.names)  # type: ignore\n        keys = set(over.keys())\n        if groups.intersection(keys):\n            raise KeyError(\"unable to overwrite group keys\")\n        df = df.agg(**over)\n        df = df.reset_index(drop=True)\n    else:\n        df = df.agg(**over)  # type: ignore\n        df = df.T  # type: ignore\n        df = df.reset_index(drop=True)  # type: ignore\n        df = df.fillna(method=\"ffill\")  # type: ignore\n        df = df.fillna(method=\"bfill\")  # type: ignore\n        df = df.head(1)  # type: ignore\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "rollup.py"], "context_start_lineno": 0, "lineno": 10, "function_name": "rollup"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def append(top: PandasDataFrame, bottom: PandasDataFrame) -> PandasDataFrame:\n    df = pd.concat([top, bottom])\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "append.py"], "context_start_lineno": 0, "lineno": 6, "function_name": "append"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def combine(\n    df: PandasDataFrame, columns: Columns, into: Column, sep: str, drop: bool = True\n) -> PandasDataFrame:\n    _check_type(columns, list)\n    _check_type(into, str)\n    _check_type(sep, str)\n    _check_type(drop, bool)\n    into_is_in_columns = into in columns\n    into_is_not_in_columns = not into_is_in_columns\n    into_is_in_df_columns = into in df.columns\n    if into_is_not_in_columns and into_is_in_df_columns:\n        message = f\"overwriting existing column '{into}'\"\n        warnings.warn(message)\n    df = df.copy()\n    new = df[columns].apply(lambda row: sep.join(row.values.astype(str)), axis=1)\n    if drop:\n        df = df.drop(columns, axis=1)\n    df[into] = new\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "combine.py"], "context_start_lineno": 0, "lineno": 11, "function_name": "combine"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def replace(\n    df: PandasDataFrame, over: dict[Column, dict[OldValue, NewValue]]\n) -> PandasDataFrame:\n    _check_type(over, dict)\n    bad_columns = list(set(over.keys()) - set(df.columns))\n    if bad_columns and len(bad_columns) == 1:\n        raise KeyError(f\"column key: {bad_columns} is invalid\")\n    if bad_columns and len(bad_columns) > 1:\n        raise KeyError(f\"column keys: {bad_columns} are invalid\")\n    df = df.replace(over)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "replace.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "replace"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def denix(df: PandasDataFrame, columns: LazyColumns | None = None) -> PandasDataFrame:\n    _check_type(columns, {list, str, None})\n    columns = [columns] if isinstance(columns, str) else columns\n    if isinstance(columns, list):\n        bad_keys = set(columns).difference(df.columns)\n        if bad_keys:\n            if len(bad_keys) == 1:\n                message = f\"columns argument contains invalid key {bad_keys}\"\n            else:\n                message = f\"columns argument contains invalid keys {bad_keys}\"\n            raise KeyError(message)\n    df = df.dropna(subset=columns)\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "denix.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "denix"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def dedupe(df: PandasDataFrame, columns: LazyColumns | None = None) -> PandasDataFrame:\n    _check_type(columns, {list, str, None})\n    _check_keys(columns, df.columns)\n    df = df.drop_duplicates(subset=columns, keep=\"first\")\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "dedupe.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "dedupe"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def accumulate(\n    df: PandasDataFrame | PandasGroupedFrame, column: Column, into: Column\n) -> PandasDataFrame:\n    _check_type(column, str)\n    _check_type(into, str)\n    if isinstance(df, PandasDataFrame):\n        into_is_not_column = into != column\n        into_is_in_df_columns = into in df.columns\n        if into_is_not_column and into_is_in_df_columns:\n            message = f\"overwriting existing column '{into}'\"\n            warnings.warn(message)\n        df = df.copy()\n    result = df[column].cumsum()\n    if isinstance(df, PandasGroupedFrame):\n        df = df.obj.copy()  # type: ignore\n    df[into] = result  # type: ignore\n    return df  # type: ignore\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "accumulate.py"], "context_start_lineno": 0, "lineno": 11, "function_name": "accumulate"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def cross(\n    lhs: PandasDataFrame,\n    rhs: PandasDataFrame,\n    postfix: tuple[str, str] = (\"_lhs\", \"_rhs\"),\n) -> PandasDataFrame:\n    _check_type(postfix, tuple)\n    df = pd.merge(lhs, rhs, how=\"cross\", suffixes=postfix)\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "cross.py"], "context_start_lineno": 0, "lineno": 13, "function_name": "cross"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def sample(\n    df: PandasDataFrame, rows: int | float, seed: int | None = None\n) -> PandasDataFrame:\n    _check_type(rows, {int, float})\n    if rows >= 1:\n        if isinstance(rows, float):\n            raise ValueError(\"must be int if > 1\")\n        df = df.sample(rows, random_state=seed)\n    elif 0 < rows < 1:\n        df = df.sample(frac=rows, random_state=seed)\n    else:\n        raise ValueError(\"must be > 0\")\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "sample.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "sample"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def shuffle(df: PandasDataFrame, seed: int | None = None) -> PandasDataFrame:\n    _check_type(seed, {int, None})\n    df = df.sample(frac=1, random_state=seed)\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "shuffle.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "shuffle"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def unpack(df: PandasDataFrame, column: Column, sep: str) -> PandasDataFrame:\n    _check_type(column, str)\n    _check_type(sep, str)\n    df = df.assign(**{column: df[column].str.split(sep)})\n    df = df.explode(column)\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "unpack.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "unpack"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def rank(\n    df: PandasDataFrame | PandasGroupedFrame,\n    column: Column,\n    into: Column,\n    descending: bool = False,\n) -> PandasDataFrame:\n    _check_type(column, str)\n    _check_type(into, str)\n    _check_type(descending, bool)\n    if isinstance(df, PandasDataFrame):\n        into_is_not_column = into != column\n        into_is_in_df_columns = into in df.columns\n        if into_is_not_column and into_is_in_df_columns:\n            message = f\"overwriting existing column '{into}'\"\n            warnings.warn(message)\n        df = df.copy()\n    result = df[column].rank(method=\"dense\", ascending=not descending)\n    if isinstance(df, PandasGroupedFrame):\n        df = df.obj.copy()  # type: ignore\n    df[into] = result  # type: ignore\n    return df  # type: ignore\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "rank.py"], "context_start_lineno": 0, "lineno": 14, "function_name": "rank"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def fill(\n    df: PandasDataFrame,\n    columns: LazyColumns | None = None,\n    direction: Direction | None = None,\n    constant: Value | None = None,\n) -> PandasDataFrame:\n    _check_type(columns, {list, str, None})\n    _check_type(direction, {str, None})\n    columns = [columns] if isinstance(columns, str) else columns\n    if (direction != None) and (constant != None):\n        raise ValueError(\"either direction OR constant must be None\")\n    if (direction == None) and (constant == None):\n        raise ValueError(\"either direction OR constant must not be None\")\n    if direction != None:\n        if not (direction in [\"down\", \"up\"]):\n            raise ValueError(\"must be one of {'down', 'up'}\")\n        method = {\"down\": \"ffill\", \"up\": \"bfill\"}.get(direction)\n        value = None\n    if constant != None:\n        value = constant\n        method = None\n    df = df.copy()\n    if columns:\n        df[columns] = df[columns].fillna(value=value, method=method)  # type: ignore\n    else:\n        df = df.fillna(value=value, method=method)  # type: ignore\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "fill.py"], "context_start_lineno": 0, "lineno": 12, "function_name": "fill"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def spread(df: PandasDataFrame, column: Column, using: Column) -> PandasDataFrame:\n    _check_type(column, str)\n    _check_type(using, str)\n    if column == using:\n        raise KeyError(\"column and using must be unique\")\n    original_shape = df.shape[1]\n    if original_shape == 2:\n        temp = uuid.uuid4().hex\n        df[temp] = df.groupby(column).cumcount()\n    index = [col for col in df.columns if col not in [column, using]]\n    df = pd.pivot_table(df, index=index, columns=[column], values=[using], aggfunc=\"first\")  # type: ignore\n    df.columns = [col for col in df.columns.get_level_values(1)]  # type: ignore\n    df = df.reset_index().rename_axis(None, axis=0)\n    if original_shape == 2:\n        df = df.drop(temp, axis=1)  # type: ignore\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "spread.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "spread"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _melt(\n    df: PandasDataFrame,\n    cols_to_keep: list[str],\n    cols_to_gather: list[str],\n    into: tuple[str, str],\n) -> PandasDataFrame:\n    df = pd.melt(\n        df,\n        id_vars=cols_to_keep,\n        value_vars=cols_to_gather,\n        var_name=into[0],\n        value_name=into[1],\n    )\n    df = df.dropna(subset=into[1])  # type: ignore\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "gather.py"], "context_start_lineno": 0, "lineno": 16, "function_name": "_melt"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _grouped_melt(df: PandasGroupedFrame, into: tuple[str, str]) -> PandasDataFrame:\n    cols_to_keep = df.grouper.names  # type: ignore\n    cols_to_gather = [col for col in df.obj.columns if col not in cols_to_keep]  # type: ignore\n    df = _melt(df.obj, cols_to_keep, cols_to_gather, into)  # type: ignore\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "gather.py"], "context_start_lineno": 0, "lineno": 29, "function_name": "_grouped_melt"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def join(\n    lhs: PandasDataFrame,\n    rhs: PandasDataFrame,\n    on: LazyColumns,\n    how: Join = \"left\",\n    postfix: tuple[str, str] = (\"_lhs\", \"_rhs\"),\n) -> PandasDataFrame:\n    _check_type(on, {list, str})\n    _check_type(how, str)\n    _check_type(postfix, tuple)\n    if not how in [\"left\", \"right\", \"inner\", \"full\"]:\n        message = (\n            \"on argument is invalid, must be one of {'left', 'right', 'inner', 'full'}\"\n        )\n        raise ValueError(message)\n    how = \"outer\" if (how == \"full\") else how  # type: ignore\n    df = pd.merge(lhs, rhs, on=on, how=how, suffixes=postfix)\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "join.py"], "context_start_lineno": 0, "lineno": 15, "function_name": "join"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def filter(df: PandasDataFrame, func: Func) -> PandasDataFrame:\n    if not callable(func):\n        raise TypeError(\"must be Func\")\n    df = df.loc[func]  # type: ignore\n    df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "filter.py"], "context_start_lineno": 0, "lineno": 4, "function_name": "filter"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def take(\n    df: PandasDataFrame | PandasGroupedFrame, rows: int = 1, **kwargs\n) -> PandasDataFrame:\n    if kwargs:  # compatibility: sklearn / train_test_split\n        df = df.take(rows, **kwargs)  # type: ignore\n        df = df.reset_index(drop=True)\n        return df\n    _check_type(rows, int)\n    if isinstance(df, PandasDataFrame):\n        if rows > df.shape[0]:\n            raise ValueError(\"rows argument is invalid, exceeds total size\")\n    if rows == 0:\n        raise ValueError(\"rows argument is invalid, must not be 0\")\n    if rows <= -1:\n        df = df.tail(rows * -1)\n    else:\n        df = df.head(rows)\n    if isinstance(df, PandasGroupedFrame):\n        df = df.reset_index()\n    else:\n        df = df.reset_index(drop=True)\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "take.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "take"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def group(df: PandasDataFrame, by: LazyColumns) -> PandasGroupedFrame:\n    _check_type(by, {list, str})\n    gdf = df.groupby(by, as_index=False, sort=False)\n    return gdf\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "group.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "group"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def select(df: PandasDataFrame, columns: LazyColumns) -> PandasDataFrame:\n    _check_type(columns, {list, str})\n    columns = [columns] if isinstance(columns, str) else columns\n    if len(set(columns)) != len(columns):\n        raise KeyError(f\"column keys must be unique\")\n    bad_columns = list(set(columns) - set(df.columns))\n    if bad_columns and len(bad_columns) == 1:\n        raise KeyError(f\"column key: {bad_columns} is invalid\")\n    if bad_columns and len(bad_columns) > 1:\n        raise KeyError(f\"column keys: {bad_columns} are invalid\")\n    df = df[columns]\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "verbs", "select.py"], "context_start_lineno": 0, "lineno": 7, "function_name": "select"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def save(df: DataFrame, path: str, **kwargs) -> None:\n    \"\"\"Save a rf.DataFrame to a csv file (opposite of `load`)\n\n    Example:\n\n    ```python\n    rf.save(df, \"example.csv\")\n    ```\n    \"\"\"\n    _check_type(df, DataFrame)\n    _check_type(path, str)\n    _check_file(path)\n    df._data.to_csv(path, index=False, **kwargs)\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "io", "save.py"], "context_start_lineno": 0, "lineno": 13, "function_name": "save"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def load(path: str, **kwargs) -> DataFrame:\n    \"\"\"Load a csv file into a rf.DataFrame (opposite of `save`)\n\n    Example:\n\n    ```python\n    df = rf.load(\"example.csv\")\n    ```\n    \"\"\"\n    _check_type(path, str)\n    _check_file(path)\n    data: PandasDataFrame = pd.read_csv(path, **kwargs)  # type: ignore\n    _check_index(data)\n    _check_columns(data)\n    return _wrap(data)\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "io", "load.py"], "context_start_lineno": 0, "lineno": 17, "function_name": "load"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def wrap(pdf: PandasDataFrame) -> DataFrame:\n    \"\"\"Convert a pd.DataFrame into a rf.DataFrame (opposite of `unwrap`)\n\n    Example:\n\n    ```python\n    pdf = pd.DataFrame({\"foo\": range(10)})\n    rdf = rf.wrap(pdf)\n    ```\n    \"\"\"\n    _check_type(pdf, PandasDataFrame)\n    _check_index(pdf)\n    _check_columns(pdf)\n    rdf = DataFrame()\n    rdf._data = pdf.copy()\n    return rdf\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "io", "convert.py"], "context_start_lineno": 0, "lineno": 31, "function_name": "wrap"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _wrap(data: PandasDataFrame) -> DataFrame:\n    \"\"\"Unsafe version of redframes.io.wrap()\"\"\"\n    df = DataFrame()\n    df._data = data\n    return df\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 0, "lineno": 58, "function_name": "_wrap"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def summarize(self, over: dict[Column, tuple[Column, Func]]) -> DataFrame:\n        message = \"Marked for removal, please use `rollup` instead\"\n        warnings.warn(message, FutureWarning)\n        return self.rollup(over)\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 142, "lineno": 358, "function_name": "summarize"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def __init__(self, data: dict[Column, Values] | None = None) -> None:\n        \"\"\"Initialize a DataFrame with a standard dictionary\n\n        Example:\n\n        ```python\n        df = rf.DataFrame({\"foo\": [1, 2], \"bar\": [\"A\", \"B\"]})\n        ```\n        |   foo | bar   |\n        |------:|:------|\n        |     1 | A     |\n        |     2 | B     |\n        \"\"\"\n        _check_type(data, {dict, None})\n        if not data:\n            self._data = PandasDataFrame()\n        if isinstance(data, dict):\n            self._data = PandasDataFrame(data)\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 177, "lineno": 387, "function_name": "__init__"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def __eq__(self, rhs: Any) -> bool:\n        \"\"\"Check if two DataFrames are equal to each other\n\n        Example:\n\n        ```python\n        adf = rf.DataFrame({\"foo\": [1]})\n        bdf = rf.DataFrame({\"bar\": [1]})\n        cdf = rf.DataFrame({\"foo\": [1]})\n        print(adf == bdf)\n        print(adf == cdf)\n        # False\n        # True\n        ```\n        \"\"\"\n        if not isinstance(rhs, DataFrame):\n            return False\n        return self._data.equals(rhs._data)\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 197, "lineno": 408, "function_name": "__eq__"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def __str__(self) -> str:\n        \"\"\"Return string constructor (for copy-and-pasting)\n\n        Example:\n\n        ```python\n        df = rf.DataFrame({\"foo\": [1, 2], \"bar\": [\"A\", \"B\"]})\n        str(df)\n        # \"rf.DataFrame({'foo': [1, 2], 'bar': ['A', 'B']})\"\n        ```\n        \"\"\"\n        data = self._data.to_dict(orient=\"list\")\n        string = pprint.pformat(data, indent=4, sort_dicts=False, compact=True)\n        if \"\\n\" in string:\n            string = \" \" + string[1:-1]\n            string = f\"rf.DataFrame({{\\n{string}\\n}})\"\n        else:\n            string = f\"rf.DataFrame({string})\"\n        return string\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 227, "lineno": 442, "function_name": "__str__"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def memory(self) -> str:\n        \"\"\"Interrogate DataFrame (deep) memory usage\n\n        Example:\n\n        ```python\n        df = rf.DataFrame({\"foo\": [1, 2, 3], \"bar\": [\"A\", \"B\", \"C\"]})\n        df.memory\n        # '326B'\n        ```\n        \"\"\"\n        size = self._data.memory_usage(deep=True).sum()\n        power_labels = {40: \"TB\", 30: \"GB\", 20: \"MB\", 10: \"KB\"}\n        for power, label in power_labels.items():\n            if size >= (2**power):\n                approx_size = size // 2**power\n                return f\"{approx_size} {label}\"\n        return f\"{size} B\"\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 277, "lineno": 505, "function_name": "memory"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def types(self) -> dict[Column, type]:\n        \"\"\"Inspect column types\n\n        Example:\n\n        ```python\n        df = rf.DataFrame({\"foo\": [1, 2], \"bar\": [\"A\", \"B\"], \"baz\": [True, False]})\n        df.types\n        # {'foo': int, 'bar': object, 'baz': bool}\n        ```\n        \"\"\"\n        numpy_types = {\n            NumpyType(\"O\"): object,\n            NumpyType(\"int64\"): int,\n            NumpyType(\"float64\"): float,\n            NumpyType(\"bool\"): bool,\n            NumpyType(\"datetime64\"): DateTime,\n        }\n        raw_types = dict(self._data.dtypes)\n        clean_types = {}\n        for column in self.columns:\n            current = raw_types[column]\n            clean = numpy_types.get(current, current)  # type: ignore\n            clean_types[column] = clean\n        return clean_types\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 306, "lineno": 525, "function_name": "types"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def cross(\n        self, rhs: DataFrame | None = None, postfix: tuple[str, str] = (\"_lhs\", \"_rhs\")\n    ) -> DataFrame:\n        \"\"\"Cross join columns from another DataFrame\n\n        Examples:\n\n        ```python\n        df = rf.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [1, 2, 3]})\n        ```\n        | foo   |   bar |\n        |:------|------:|\n        | a     |     1 |\n        | b     |     2 |\n        | c     |     3 |\n\n        Self:\n\n        ```python\n        df.cross()\n        ```\n\n        | foo_lhs   |   bar_lhs | foo_rhs   |   bar_rhs |\n        |:----------|----------:|:----------|----------:|\n        | a         |         1 | a         |         1 |\n        | a         |         1 | b         |         2 |\n        | a         |         1 | c         |         3 |\n        | b         |         2 | a         |         1 |\n        | b         |         2 | b         |         2 |\n        | b         |         2 | c         |         3 |\n        | c         |         3 | a         |         1 |\n        | c         |         3 | b         |         2 |\n        | c         |         3 | c         |         3 |\n\n        Two DataFrames:\n\n        ```python\n        dfa = rf.DataFrame({\"foo\": [1, 2, 3]})\n        dfb = rf.DataFrame({\"bar\": [1, 2, 3]})\n        dfa.cross(dfb, postfix=(\"_a\", \"_b\"))\n        ```\n\n        |   foo |   bar |\n        |------:|------:|\n        |     1 |     1 |\n        |     1 |     2 |\n        |     1 |     3 |\n        |     2 |     1 |\n        |     2 |     2 |\n        |     2 |     3 |\n        |     3 |     1 |\n        |     3 |     2 |\n        |     3 |     3 |\n        \"\"\"\n        rhs = self if (rhs == None) else rhs\n        _check_type(rhs, DataFrame)\n        return _wrap(cross(self._data, rhs._data, postfix))  # type: ignore\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "core.py"], "context_start_lineno": 431, "lineno": 653, "function_name": "cross"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _check_type(argument: Any, against: type | set[type | None]) -> None:\n    if isinstance(against, set):\n        if len(against) == 0:\n            against = {against}  # type: ignore\n    if not isinstance(against, set):\n        against = {against}\n    optional = None in against\n    just_types = against.difference({None})\n    checks = [isinstance(argument, t) for t in just_types]  # type: ignore\n    if optional:\n        checks += [argument == None]\n    if not any(checks):\n        str_types = \" | \".join([t.__name__ for t in just_types])  # type: ignore\n        if optional:\n            str_types += \" | None\"\n        raise TypeError(f\"must be {str_types}\")\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "checks.py"], "context_start_lineno": 0, "lineno": 13, "function_name": "_check_type"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _check_keys(columns: LazyColumns | None, against: Columns | PandasIndex) -> None:\n    if isinstance(columns, str):\n        columns = [columns]\n    columns = [] if (columns == None) else columns\n    bad_keys = set(columns).difference(against)  # type: ignore\n    if bad_keys:\n        if len(bad_keys) == 1:\n            raise KeyError(f\"invalid key {bad_keys}\")\n        else:\n            raise KeyError(f\"invalid keys {bad_keys}\")\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "checks.py"], "context_start_lineno": 0, "lineno": 36, "function_name": "_check_keys"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _check_index(df: PandasDataFrame) -> None:\n    if not (df.index.name == None):\n        raise IndexError(\"must be unnamed\")\n    if not isinstance(df.index, PandasRangeIndex):\n        raise IndexError(\"must be range\")\n    if not (df.index.start == 0):\n        raise IndexError(\"must start at 0\")\n    if not (df.index.step == 1):\n        raise IndexError(\"must step by 1\")\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "checks.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "_check_index"}}
{"metadata": {"task_id": "maxhumber--redframes/idx", "ground_truth": "def _check_columns(df: PandasDataFrame) -> None:\n    if type(df.columns) != PandasIndex:\n        raise KeyError(\"must be flat\")\n    if df.columns.has_duplicates:\n        raise KeyError(\"must not contain duplicate keys\")\n", "fpath_tuple": ["maxhumber_redframes", "redframes", "checks.py"], "context_start_lineno": 0, "lineno": 59, "function_name": "_check_columns"}}
